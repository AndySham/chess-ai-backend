\chapter{Interpretability}

\section{Introduction}

Research into problems in Machine Learning over the past two decades has focused largely into using Neural Network (NN) models to solve an increasingly large breadth of problems. NNs, much like many other models, define a hypothesis class of functions which differ only in their parameterisation, and uses Stochastic Gradient Descent (SGD) or some derivative thereof, to optimise said parameters given a loss function. Classical Multi-Layer Perceptrons (MLPs) consist of a series of linear layers separated by some non-linearity, (e.g. ReLU, Sigmoid functions). Given certain conditions, it is known that MLPs can learn any continuous function to arbitrary precision, by the Universal Approximation Theorem (UAT). The effectiveness of SGD methods allows us to learn arbitrary functions tractably, which has resulted in a widespread adoption of the architecture in practical settings.

A common criticism of NNs, however, is that they are considered ``black box" functions. NNs are difficult to interpret, making it even more difficult to diagnose issues that may arise in production. A particular node in the network may be considered as capturing a single ``concept", which can further be used to determine a metric for the presence of other concepts. It is difficult, however, to intuit how these concepts are generated - taking linear combinations of features and then applying a non-linear map to the result is not a terribly human line of thinking when it comes to pattern recognition. In this way, when comparing NNs to real-world neural processes, the description of NNs capturing general human intuition, rather than any kind of conscious reasoning, is most apt.

The ``black box" nature of NNs has resulted in a hesitancy for it's adoption in particular settings, most notably in the medical industry, where even small risks of misdiagnosis cannot be tolerated. One would assume that an architecture that is so widely used in so many sensitive settings should be easily examinable, but this isn't that case.

From this, the notion of ``interpretability" as an important concept in ML was introduced. An ideal interpretable model would allow the user to know precisely what the model is achieving by arriving at a particular optimal parameterisation. Interpretability is not a quantifiable metric - the user may gain an understanding through a mixture of the learnt parameters and an existing intuition over the architecture itself. This allows for a wide breadth in methods which may be used to gain an understanding of a model, and also hopefully the underlying problem.

\section{Interpretation Methods}

There are many ways we may attempt to approach the problem of interpretability. Commonly used methods take arbitrary NN architectures, and attempt to gauge how relevant a particular feature of a given input is in the overall output of the model. These are known as \textit{Variable Importance Methods} (VIMs). Gradient-based attribution methods \cite{gradientattribute} are the classical example, where the gradient of the model with respect to it's input features are used as the measure of feature importance. This makes intuitive sense, as if the output of the model is subject to large changes with small deviations in an input feature, it must naturally be fairly important. The most commonly seen setting for these methods is in image classification, where the importance of a particular pixel measures how much of an influence said pixel has on determining the category of an image. Plotting the importance of all the pixels hopefully shows the user precisely which portions of the image contain the relevant object to classification. E.g., distinguishing between cats and dogs would largely rely on examining particular features of the face shape, so one would expect these features to be the most important by this metric.

These methods are very versatile, as they are \textit{model-agnostic}. There are many flaws, however - what if the classification of an image relies on a combination of features, rather than just a single one? We can capture this notion by instead using VIMs over all nodes in the network, i.e. the input layers and all hidden layers, but we run into the same problem - if we determine that a node in a hidden layer is important, how do we begin to understand what this node is doing? This method captures relevance, but does not capture what concepts these features may represent - we can leave this again up to the intuition of the user, or we can apply VIMs recursively between the input features and the hidden feature. This eventually becomes somewhat unwieldy. 

Another issue is that VIMs are \textit{local} interpretation methods, as they do not describe the model as a whole - only the model given a set input. This does not give us a good understanding as to why the model's solution to a problem is best.

A solution to the problem of not capturing feature relationships is in developing \textit{model-specific} methods of interpretability. We can design an architecture which allows for novel ways of visualising model behaviour, often by restricting the expressiveness of the model in a manner which allows the remaining hypothesis class to be easily distinguishable. 

One example of such an architecture are Neural Additive Models (NAMs) \cite{neuraladditive}. Neural Additive Models are a generalisation of General Additive Models (GAMs) in that they are fully described by the equation
$$M(\mathbf{x}) = \sigma\left(\sum_i M_i(x_i)\right)$$

Where the $M_i:\R \mapsto \R$ represent univariate NNs, and $\sigma$ is the \textit{link function}.

In backpropagation, we learn the parameters of each subnetwork $M_i$ simultaneously. Given that each subnetwork is a map $\R \to \R$, we can capture the behaviour of the model not only locally through VIMs, but globally, as we can easily plot the value of $M_i$ over the entire domain. Simply observing this graph allows the user to speculate as to what the model has learnt.

This model, while very interpretable, is not very expressive - we cannot capture any relationships between variables that aren't described by the link function $\sigma$, as is the nature of GAMs. This is the very problem we intended to solve by discussing NAMs - we want to be able to capture not only the relevance of input features, but of learnt concepts over those features.

Again, new model architectures have been introduced to resolve this. The aptly named Explainable Neural Network (xNN) \cite{xnn} is an architecture which extends NAMs with a single linear ``projection layer". These are equivalent to learning a GAM over \textit{linear combinations} of input features. They are therefore fully described by the equations
$$
\begin{aligned}
\mathbf{a} &= \mathbf{Wx + b}\\
M(\mathbf{x}) &= \sigma\left(\sum_i M_i(a_i)\right)
\end{aligned}
$$

Where $\mathbf{W}, \mathbf{b}$ are learnable parameters as standard. We can interpret this model very similarly, by again plotting the univariate subnetworks $M_i$, and simply interpreting what a concept $a_i$ represents by directly observing the linear combination. The UAT tells us that this single added layer is enough to model any function to an arbitrary precision, but in practice this hidden layer would need to be quite wide for anything other than the most trivial problems, and the features introduced in a large hidden layer may not be terribly intuitive to understand, and may even introduce a large bias.

We could extend this further, by adding more perceptron layers to capture more complicated relations between input features, but this naturally comes at the expense of interpretability. An ideal solution to this problem would allow for the model to be extended with more layers without sacrifice.

This motivates a new architecture for layers in our NN, as perceptron layers are the main obstruction to interpreting our models.

\section{Inspiration from Non-NN Architectures}

AI research is not a field that began with the development of NNs. Early research focused mainly on the nature of logical statements - put simply, if we assume facts $P_1, \dots, P_n$, what further facts can we derive? A simple example is given by; if we know $A$ and $A \rightarrow B$ to be true, then $B$ immediately follows.

Given a set of background knowledge facts $P = P_1 \land \dots \land P_n$, sets of \textit{positive examples} $E^+=E^+_1\land \cdots$, and \textit{negative examples} $E^-=\lnot E^-_1 \land \cdots$, it would be useful to determine a hypothesis $H$ such that;
$$
\begin{aligned}
    \text{(Necessity) } &P \centernot\implies E^+ \\
    \text{(Sufficiency) } &P \land H \implies E^+ \\
    \text{(Weak Consistency) } &P \land H \centernot\implies \text{False} \\ 
    \text{(Strong Consistency) } &P \land H \land E^- \centernot\implies \text{False} \\
\end{aligned}
$$

Intuitively, the necessity and sufficiency conditions ensure that we can verifiably prove $E^+$ with $H$, but not without. Weak consistency forces $H$ to not result in a contradiction (as this would entail every logical statement), and strong consistency requires $H$ not to prove anything in $E^-$ that we assert not to be true.

A program that can compute answers to the above problem is known as an Inductive Logic Programming (ILP) system. ILP systems are very useful - suppose we know of a particular game concept which is beneficial to the player but for which we have no logical representation. However, we may have $E^+$ as positive examples, and $E^-$ as negative examples. If we can determine an $H$ which satisfies the above conditions, that is, $H$ is a logical statement that is consistent with our set of examples, then we can use this learnt $H$ to describe the concept as a whole, and extrapolate our findings to game states we have not yet seen. We can therefore incorporate $H$ into a heuristic we may use in evaluating a game state, which can go on to be used in more sophisticated algorithms such as the breadth of variants of Min-Max tree search.

A further benefit of ILP systems is that they are by nature interpretable. The hypothesis $H$ is a single logical statement which can easily be read and verified by humans. Therefore if we do manage to devise an ILP system, and corresponding heuristic, we could learn to play a game in a highly interpretable manner. The goal of this paper is to come up with a system similar in nature to this.

Symbolic inference, like the ILP systems described above, formed the bulk of research into AI and Machine Learning from it's early inception in the 1950s up until the 1990s. This is notably no longer the case - ever since the 2000s, Machine Learning as a field has become incredibly publicly prominent through the development of statistical ML methods, and most notably NNs. NNs proved to be much more adept at learning behaviours given smaller amounts of data, where in practice, symbolic machine learning requires an incredibly large amount of data to derive anything meaningful. This lead to famous criticisms, such as those levelled by Hubert Dreyfus, as to whether the field of symbolic machine learning would be suitable for anything other than simple toy problems.  

If we want to leverage the learning power of NNs, and the interpretability of ILP systems, we need to find a way of compromising the two approaches. That is, we want to create an ILP system which learns in the same way as NNs, through backpropagation. There is immediately an obvious issue - logical hypotheses created by ILP systems are \textit{boolean functions}, that is they are mappings $\{0,1\}^n \to \{0,1\}$. Backpropagation relies on computing derivatives of a model described as a function over a continuous domain - which $\{0,1\}^n$ is distinctly not. If we want to begin devising methods of backpropagation over boolean functions, we have to solve this issue. 