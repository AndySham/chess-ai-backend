\documentclass[conference]{report}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm, dsfont}
\usepackage{centernot}
\begin{document}

\def\T{\textbf{T}}
\def\F{\textbf{F}}
\def\NOT{\texttt{NOT}}
\def\AND{\texttt{AND}}
\def\OR{\texttt{OR}}
\def\XOR{\texttt{XOR}}
\def\R{\mathbb{R}}
\def\B{\mathbb{B}}
\def\N{\mathbb{N}}

\def\custop#1{{\mspace{4mu}#1\mspace{4mu}}}
\def\concat{\custop{@}}

\title{\LARGE Designing Interpretable Chess Engines Using Logical Neural Networks}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
The problem of interpretability in DNNs is classically a hard one, as the parameterisation of network layers is hard to intuit. This paper uses findings in Logical Neural Network architectures to construct an interpretable model for use cases which are easily modelled by logical statements, and applies this architecture to the problem of learning Chess.  
\end{abstract}

\tableofcontents

\pagebreak

\include{c1.tex}
\include{c2.tex} 
\include{c3.tex}

\chapter{A Chess Architecture}

\chapter{Interpreting Chess}

\chapter{Conclusions}
 

\bibliography{bib.bib}


%\begin{thebibliography}{1}
%\bibitem {gradientattribute}
%\href{run:https://link.springer.com/chapter/10.1007/978-3-030-28954-6_9}{Ancona M., Ceolini E., Ã–ztireli C., Gross M. (2019) Gradient-Based Attribution Methods. \emph{Explainable AI: Interpreting, Explaining and Visualizing Deep Learning.} Lecture Notes in Computer Science, vol 11700. Springer, Cham. https://doi.org/10.1007/978-3-030-28954-6\_9}

%\bibitem {xnn}
%\href{run:https://link.springer.com/chapter/10.1007/978-3-030-28954-6_9}{J. Vaughan, A. Sudjianto, E. Brahimi, J. Chen, V. Nair. Explainable Neural Networks based on Additive Index Models. arXiv:1806.01933}

%\end{thebibliography}
\end{document}