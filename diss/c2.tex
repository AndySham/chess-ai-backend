
\chapter{Logical Neural Networks}

\section{Introduction}

The field of research which focuses on deriving methods of backpropagation over boolean functions is referred to as \textit{Neurosymbolic Machine Learning}. The models that are created to solve such a problem are known as \textit{Logical Neural Networks}\footnote{also \textit{Neural Logic Networks}} (LNNs)\footnote{also NLNs}. 

The natural solution to solving the problem of $\{0,1\}$ being a discrete space, is by embedding values $\T$, $\F$ in a continuous space (or more formally, a topological manifold), and computing derivatives in this space instead. This poses an obvious problem - what if we find that the optimal parameters are values that do not equal $\T$ or $\F$? The ways we may begin to solve this problem differ depending on the particular architecture we decide to use. First, we must provide some background information as to how we may begin to do so in the first place.

%From now on, we will begin using the symbol $\B$ when referring to the space of boolean representations for a given architecture. Where two modes of representation are being compared, they will be differentiated with a subscript (e.g. $\B_2$, $\B_\text{f}$, etc.)

%We will explicitly refer to the set $\{0,1\}$ used in classical logic with notation $\B_2$.

\section{A Useful Logical System}

\subsection{Notation}

The notation of linear algebra is very useful in succinctly describing the nature of a classical perceptron model, but within neurosymbolic layers, we do not have this luxury. Further in this paper, we will discuss many different kinds of logical systems, and we will discuss and compare their qualities in learning. To describe our models, it's useful to phrase the layers we construct in a manner similar to linear algebra.

Given a logical system $\B$, we will define a matrix/vector system much like the standard one. A boolean vector of size $n$ is an element of the set $\B^n$, and similarly a boolean matrix of height $a$ and width $b$ is an element of the set $\B^{a \times b}$. Where we now differ is in matrix algebra - in linear algebra, matrix multiplication is defined like so;
$$\mathbf{AB}_{ij} = \sum_{k}\mathbf{A}_{ik}\cdot\mathbf{B}_{kj}$$

We can define boolean matrix multiplication likewise;
$$\mathbf{AB}_{ij} = \bigvee_{k}\mathbf{A}_{ik}\land\mathbf{B}_{kj}$$

This allows us to motivate a meaning for ``dot products" also;
$$\mathbf{a\cdot b} = \mathbf{a}^\text{T}\mathbf{b} = \bigvee_i a_i \land b_i$$

It would be useful to define element-wise operations on vectors/matrices. For any operator $\circ : \B^2 \to \B$, we write $\mathbf{a} \circ \mathbf{b}$ to mean $(\mathbf{a} \circ \mathbf{b})_i = a_i \circ b_i$. If we want to apply an operator with a constant scalar value to every element of a vector, we can write this likewise, $(\mathbf{a} \circ b)_i = a_i \circ b$, etc.

Finally, if an operator $\circ$ is both associative and commutative, we write $^\circ(\mathbf{a})$ to refer to $a_1 \circ a_2 \circ \cdots$.

Using the above two, we can likewise define the ``dot product" by $\mathbf{a \cdot b} ={^\lor}(\mathbf{a \land b})$. We can use a similar notation as syntactic sugar to define matrix multiplication $\mathbf{AB} ={^\lor}(\mathbf{A \land B})$. This notation allows us to use other operators also, for example standard matrix multiplication would be written by ${^+}(\mathbf{A \times B})$. 

\subsection{Boolean Algebras}

While classical perceptron layers heavily rely on linear algebra, neurosymbolic layers cannot. Instead, we must rely on \textit{boolean algebra} to motivate our models. Boolean algebra is fundamentally different to any algebra motivated by classical arithmetic, as we do not have access to the regular operators $+$ or $\times$. Nevertheless, boolean operators behave much like arithmetic ones. A boolean algebra, formally speaking, is a mathematical object $\B$, which contains elements $\T$, $\F$, and for which we have binary operators $\lor$, $\land$, and a unary operator $\neg$, such that the following axioms always hold.

$$
\begin{aligned}
\text{(Associativity)}
&\ (a \lor b) \lor c = a \lor (b \lor c) \\
&\ (a \land b) \land c = a \land (b \land c) \\
\text{(Commutativity)}
&\ a \lor b = b \lor a \\
&\ a \land b = b \land a \\
\text{(Absorption)}
&\ a \lor (a \land b) = a \\
&\ a \land (a \lor b) = a \\
\text{(Identity)}
&\ a \lor \F = a \\
&\ a \land \T = a \\
\text{(Compliments)}
&\ a \lor \neg a = \T \\
&\ a \land \neg a = \F \\
\text{(Distributivity)}
&\ a \lor (b \land c) = (a \lor b) \land (a \lor c) \\
&\ a \land (b \lor c) = (a \land b) \lor (a \land c) \\
\end{aligned}
$$

We can immediately see the similarities to classical arithmetic. In fact, the above construct is a commutative ring, with some extra properties. 

The above system is very intuitive, as it by design can fully motivate a propositional logic. However, we have not motivated our logical system by direct construction, but by necessitating certain behaviours. We therefore need not restrict ourselves to simply the set $\B=\{0,1\}$, we can define $\B$ however we'd like, and exploit the unique characteristics of the $\B$ we have chosen in our learning.

Let's consider an alternative boolean algebra - given a set $S$, let $\B = {\{ U \subseteq S \}}$, that is, the power set of $S$. It is natural to let $\land$ be the intersection of two sets, and $\lor$ the union. We can further say $\T = S$, $\F = \varnothing$, and $\neg:U \mapsto S \setminus U$. One can immediately see that the above axioms follow from basic facts in set theory. This construction is not useful to us, however, as we cannot differentiate over it.

In practice, we'd really like not to stray too far away from classical logic $\B = \{0,1\}$, and simply approximate classical logic with our choice of $\B$. In this way, our goal is not to necessarily to exploit the properties of a boolean algebra once we have them, but to use the axioms of boolean algebras (and derivations thereof) to measure the quality of hypothetical boolean approximations we propose.

\subsection{Measuring Crispness}

To somehow measure how well our model is behaving like a boolean function, it would be useful to construct a distance metric within our space $\B$. The further away from ideal values $\T$, $\F$ we are, or from basic facts true of all boolean algebras, the worse our model may become to intuit as a logical formula.

There exist well understood ways of doing so already. A metric space $X$ is a set endowed with a \textit{distance metric} $d: X^2 \to R$ with the following properties;

$$
\begin{aligned}
\text{(Identity of Indiscernables)}
&\ d(a, b) = 0 \iff a = b \\
\text{(Symmetry)}
&\ d(a, b) = d(b, a) \\
\text{(Triangle Inequality)}
&\ d(a, b) + d(b, c) \leq d(a, c) \\
\end{aligned}
$$

For example, over classical booleans $\B = \{0,1\}$, a valid metric is $d(a,b) = a +_2 b$, where $+_2$ is addition in the finite field $\mathbb{F}_2$. This can also be interpreted as the function $\XOR(a,b) = (\neg a \land b) \lor (a \land \neg b)$. For boolean vectors $\{0,1\}^n$, a valid distance metric would be the \textit{Hamming distance} $d(\mathbf{a,b}) = {^+}(\mathbf{a} +_2 \mathbf{b})$.

For any boolean algebra $\B$, given $d$, we can define a \textit{vagueness metric}
$$v_d(a) = \min\{d(a, \T), d(a, \F)\}$$

In cases where the choice of $d$ is obvious, we will omit the subscript. We refer to values that have low vagueness as being \textit{crisp}.

\section{A General Neurosymbolic Architecture}

We will see particular implementations of LNNs later. Initially, it is important to discuss some general ideas.

As mentioned previously, the UAT shows us that it is possible to model any boolean function within the framework of MLPs, but we want to restrict the architecture of MLPs such that we could begin to interpret the learnt model as a logical formula, while maintaining full expressiveness. It is well known that any boolean function can be represented in Disjunctive Normal Form (DNF). That is, a formula that takes the form;
$$
\begin{aligned}
\phi(x_1, \dots, x_N) =\ &(a_{11} \land a_{12} \land \dots \land a_{1n_1}) \\
\lor\ &(a_{21} \land a_{22} \land \dots \land a_{2n_2}) \\ 
\lor\ &\cdots \\
\lor\ &(a_{m1} \land a_{m2} \land \dots \land a_{mn_m})
\end{aligned}
$$

where the $a_{ij} \in \{x_1, \dots, x_N, \lnot x_1, \dots, \lnot x_N\}$.

This is very convenient, as it means that any architecture we choose to build can learn this highly regular form, instead of some arbitrarily deep tree of logical operators. To transform this into the language of NNs, we want to somehow decompose the above form into a series of ``logical layers". 

There is a natural homomorphism between subsets $W \subseteq \{1, \cdots, N\}$ and disjunctions $D : \{0,1\}^n \to \{0,1\}$. That is, any disjunction $D$ is fully defined by some set $W$, where
$$D_W(\mathbf{x}) = \bigvee_{i\in W} x_i$$.

If we want to learn the disjunction $D$, it is therefore equivalent to learn the membership of the set $S$. Let $\mathbf{w} \in \B^N$, with $w_i = \mathbbm{1}(i \in S) \in \B$. Then we need only learn the value of the vector $\mathbf{w}$. It is important to note that, in the space $\B = \{0,1\}$, we have an equivalent representation

$$
\begin{aligned}
D_W(\mathbf{x}) 
&= \left(\bigvee_{i \in W} x_i\right) \lor \left(\bigvee_{i \not\in W} \F\right) \\
&= \bigvee_{i} x_i \land w_i \\
&= {^\lor}(\mathbf{w \land x}) = \mathbf{w}^T\mathbf{x}
\end{aligned}
$$

To distinguish this understanding of disjunctions with the simple operation $^\lor(\mathbf{x})$, we will refer to these as \textit{weighted disjunctions}.

This definitely justifies the notation introduced in the previous section. A similar procedure gives us a compact representation of conjunctions, though it is not as minimal;

$$
\begin{aligned}
C_W(\mathbf{x}) 
&= \bigwedge_{i \in W} x_i \\
&= \left(\bigwedge_{i \in W} x_i\right) \land \left(\bigwedge_{i \not\in W} \T \right) \\
&= \bigwedge_i \neg w_i \lor x_i \\
&= {^\land}(\mathbf{w} \Rightarrow \mathbf{x})
\end{aligned}
$$

The above notation allows us to represent boolean functions in DNF, as any formula in DNF is a disjunction of \textit{weighted conjunctions} as seen above. We write this like so.

$$F(\mathbf{x; W}) = {^\lor}({^\land}(\mathbf{W \Rightarrow x \concat \neg x}))$$

Here, $@$ is an operator representing vector concatenation.

Suppose we are given feature valuations $x_1, \dots, x_N$, and we are also given whether said features satisfy some set predicate $P(x_1, \dots, x_N)$. To create a functioning ILP system, it is enough to find some logical formula $\phi$ which is consistent with this predicate, meaning that for all inputs $\mathbf{x}$ we have received, $\phi(\mathbf{x}) = P(\mathbf{x})$. The purpose of LNNs is to use backpropagation to find an approximation to such a solution. By nature, NNs are ``forgetful" in that if the distribution and output value of incoming data changes, they are able to modify their parameterisation accordingly. They therefore aren't guaranteed to be consistent with all input data, but this actually becomes a benefit, rather than a hindrance, when we begin to embed LNN systems within classical NN architectures. 

This embedding is important to note, as the aim of the architecture presented in this paper is not to find \textit{correct} concepts, but \textit{useful} ones. We do not necessarily aim to find hypotheses that perfectly represent known concepts, but concepts that when incorporated into our decision making, allow the player of a game to perform effectively. In that sense, we are using LNNs in a way that is similar in construction, but different in intention to ILPs. This is an important distinction to make when comparing this architecture to existing ones in the space of Neurosymbolic ML, which we will discuss later.

\subsection{Interpreting a Neurosymbolic Layer}

We have constructed layers which represent learnable conjunctions and disjunctions, but how do we go about converting this into a human interpretable format? So far we have discussed an architecture which learns any boolean function in DNF, but is this an ideal way of communicating concepts to humans?

Let's consider what a formula in DNF is actually communicating. Each disjunction is naturally going to represent a single concept, and each conjunction within can be interpreted as being instances of said concept. Using the example of Chess, if we want to learn what it means for a bishop to attack a king, we want to iterate over all instances where a bishop is in a directly diagonal position to a king, with no obstruction. Determining each one of these instances can be done with a conjunction (e.g. Bishop on A1, empty spaces in B2, C3, ..., King on E5), and determining whether any of these conjunctions has occurred is handled by the disjunction. 

In this manner, the disjunction is a set of instances of a particular concept, and the conjunctions are the elements of this set.

Is this the best architecture for interpretability? One may consider an architecture with two DNF layers - that is, concepts building on top of further concepts. This can be incredibly useful - suppose we have both a bishop and a knight attacking the king. The presence of both of these concepts, one can imagine, is greater than the sum of it's parts, so it may be useful to consider this a joint concept. We don't necessarily require a second DNF layer to discover this, as we have shown that a single layer is sufficient to fully express all boolean functions, but for the sake of interpretability, it may be advantageous to add this second layer. This is in stark comparison to conventional NNs, where adding layers generally sacrifices interpretability for the sake of expressiveness. This consideration will become important when comparing different implementations of neurosymbolic architectures in practice.

We can now begin to discuss different ways of actually implementing such an architecture.

\section{Fuzzy Logic}

\def\Bf{\B_\text{f}}

An intuitive, and well researched approach to extending the space of valid boolean values is to consider truth to be ``vague", in the sense that something can be ``somewhat" true or ``somewhat" false. To quantify this, we take boolean values in the closed interval $[0,1]$, rather than simply $\{0,1\}$. This approach is known as \textit{fuzzy logic}.  From here, we will denote the traditional boolean algebra by $\B_2$, and fuzzy logic by $\Bf$. 

It is important to note that while this emulates the definition of a probability measure, it is distinctly not. $\mathbb{P}(x) = \frac{1}{2}$ states that $x$ is true with a probability of $\frac{1}{2}$, which can mean either that in $\frac{1}{2}$ of cases, $x$ appears true (the frequentist interpretation), or that we believe that $x$ is true with $\frac{1}{2}$ certainty (the Bayesian interpretation). In fuzzy logic, $x = \frac{1}{2}$ instead states that $x$ is ``half-true", with 100\% certainty. We are only using fuzzy logic to approximate classical, discrete logic, so it is not important to dwell on the philosophical implications of this. Extending the domain of boolean values means that we have to revisit the definitions of simple logical operators $\lnot, \land$ and $\lor$.

\subsection{Fuzzy Operators}

\def\prodand{\custop{\otimes_\times}}
\def\minand{\custop{\otimes_<}}
\def\lukand{\custop{\otimes_\text{L}}}
\def\draand{\custop{\otimes_\text{D}}}

\def\prodor{\custop{\oplus_\times}}
\def\minor{\custop{\oplus_<}}
\def\lukor{\custop{\oplus_\text{L}}}

We will begin with a generalisation for $\land$, as all further definitions follow from this. We want to find a function $\otimes : \Bf^2 \to \Bf$ which has the same value as $\land$ for $\B_2$, and maintains some natural properties of $\land$ also. Suppose we assume the following axioms for $\otimes$;
$$
\begin{aligned}
\text{(Associativity)}&\ (a \otimes b) \otimes c = a \otimes (b \otimes c) \\
\text{(Commutativity)}&\ a \otimes b = b \otimes a \\
\text{(Monotonicity)}&\ a \leq b, c \leq d \implies a \otimes c \leq b \otimes d \\
\text{(Identity)}&\ \forall a, a \otimes 1 = a
\end{aligned}
$$
 
The axioms are actually already enough to ensure that $\otimes |_{\B_2} = \land$. However, they are not enough to ensure $\otimes$ takes one particular value in the set $\B_\text{f}^2 \to \B_\text{f}$, in fact there are an infinite family of possible functions $\otimes$. The above axioms are known as the \textit{t-norm axioms}, and the functions which satisfy it are known as \textit{t-norms}. Some examples of t-norms are;
$$
\begin{aligned}
    \text{(Product t-norm)}&\ a \prodand b \coloneqq ab \\
    \text{(Minimum t-norm)}&\ a \minand b \coloneqq \min\{a,b\} \\
    \text{(Łukasiewicz t-norm)}&\ a \lukand b \coloneqq \max\{a+b-1,0\}
\end{aligned}
$$

If we can generalise either $\lnot$ or $\lor$, then we can generalise both. A seemingly obvious way to generalise $\lnot$ is by simply declaring that $\lnot x = (1 - x)$. This immediately satisfies the definition of $\lnot$ in classical logic, and comes with some useful properties;
$$
\begin{aligned}
\text{(Self-Invertibility)}&\ \lnot(\lnot a) = a \\
\text{(Monotonicity)}&\ a \leq b \implies \lnot a \geq \lnot b \\
\end{aligned}
$$

From this, we can fully define generalisations for $\lor$, which we call \textit{t-conorms}. In classical logic, we can appeal to \textit{De Morgan's laws}, in that $a \lor b = \lnot (\lnot a \land \lnot b)$. Similarly, we can say that $a \oplus b = \lnot(\lnot a \otimes \lnot b) = 1 - (1 - a) \otimes (1 - b)$, given a particular choice of t-norm $\otimes$. We therefore have;

$$
\begin{aligned}
    \text{(Product t-conorm)}\ a \prodor b \coloneqq &\ 1 - (1-a)(1-b) \\
    = &\ a + b - ab \\
    \text{(Minimum t-conorm)}\ a \minor b \coloneqq &\ 1 - \min\{1-a,1-b\} \\
    = &\ \max\{a, b\} \\
    \text{(Łukasiewicz t-conorm)}\ a \lukor b \coloneqq &\ 1 - \max\{(1-a)+(1-b)-1,0\} \\
    = &\ \min\{a+b, 1\}
\end{aligned}
$$

Given the t-norm axioms, we immediately have some convenient properties of t-conorms also.

$$
\begin{aligned}
\text{(Associativity)}&\ (a \oplus b) \oplus c = a \oplus (b \oplus c) \\
\text{(Commutativity)}&\ a \oplus b = b \oplus a \\
\text{(Monotonicity)}&\ a \leq b, c \leq d \implies a \oplus c \leq b \oplus d \\
\text{(Identity)}&\ \forall a, a \oplus 0 = a
\end{aligned}
$$

The only difference here being that the identity has value 0, rather than 1. From this, we can define every logical formula using fuzzy logic operators, allowing us to motivate an architecture for a fuzzy NN.

In the field of fuzzy logic, the above method of constructing fuzzy operators is not actually the preferred way of doing so. Fuzzy logicians instead choose to define the \textit{residuum} $\Rightarrow$ in terms of fuzzy conjunction $\otimes$. The residuum, as the symbol suggests, is meant to generalise the implication operator $(a \Rightarrow b) = \lnot(a \land \lnot b)$. The reason this can be done only relying on the existence of the t-norm (rather than t-norm + fuzzy negation) is because it can be proven that the residuum is the \textit{only} function that satisfies the conditions
$$a \otimes b \leq c \iff a \leq (b \Rightarrow c)$$

The justification for this fact comes from the understanding of fuzzy logic as measuring ``confidence" - if we have confidence valuations for $a, a \Rightarrow b \in \B_f$, and we know that $a, a \Rightarrow b$ entails $b$ in classical logic, then we would expect to be at least $a \otimes (a \Rightarrow b)$ confident in $b$, i.e. that $a \otimes (a \Rightarrow b) \leq b$. A similar construction results in the conclusion above, and allows us to uniquely determine $\Rightarrow$. From here, we can choose to define the other logical operators like so;

$$
\begin{aligned}
\lnot a =&\ (a \Rightarrow 0) \\
a \oplus b =&\ (\lnot a \Rightarrow b)
\end{aligned}
$$

Again, we will not dwell on this, as we are aiming to construct fuzzy operators specifically to approximate classical logical formulae in DNF. The definitions for negation, conjunction and disjunction given above are more than enough to begin doing so. In the notation introduced previously, we could write a formula for a fuzzy DNF layer like so.
$$
F(\mathbf{x, W}) = {^\oplus}({^\otimes}(\mathbf{W \Rightarrow x \concat \neg x}))$$

\subsection{Constructing Fuzzy Operators}

So far we have declared the existance of operators satisfying the given axioms, but we have not discussed how we might discover new ones. It is important to be able to do so freely, as the convergence rate of an algorithm may have a strong relation to the gradient of the chosen operator. To give an example, an operator known as the \textit{drastic t-norm} is 0 anywhere it doesn't have to be anything else, i.e.
$$a \draand b = \begin{cases}
    a &\text{if } b = 1 \\
    b &\text{if } a = 1 \\
    0 &\text{otherwise}
\end{cases}$$

The gradient of $\draand$ is 0 almost everywhere, which isn't terribly ideal for gradient descent. We want to be able to avoid cases like this, and to do so it would beneficial to come up with a method to construct t-norms reliably. Suppose we had a decreasing function $f:\Bf \to [0,\infty]$ with $f(1)=0$. Then the function $T(a,b) = f^{-1}(\min\{f(0), f(a) + f(b)\})$ is a t-norm - in fact, most common t-norms can be constructed in this way. This statement omits some details, but for our use cases this is fine.

A function $f$ which begets a t-norm in this manner is called an \textit{additive generator} of the t-norm. Some examples are;

$$
\begin{aligned}
\text{(Product t-norm)} &\ f(x) = -\log(x) \\
\text{(Łukasiewicz t-norm)} &\ f(x) = 1 - x \\
\text{(Drastic t-norm)} &\ f(x) = 2 - x \text{ for } x \in [0,1), f(1) = 0\\
\end{aligned}
$$

This characterisation of t-norms also allows us to explicitly define conjunctions over many variables - if $a \otimes b = f^{-1}(\min\{f(0), f(a) + f(b)\})$, then ${^\otimes}(\mathbf{x}) = f^{-1}(\min\{f(0),{^+}(f(\mathbf{x}))\})$. Thus,

$$
\begin{aligned}
^\prodand(\mathbf{x}) &= {^\times}(\mathbf{x}) \\
^\lukand(\mathbf{x}) &= \max\{0, 1 + {^+}(\mathbf{x} - 1)\}
\end{aligned}
$$


and so on. In the construction of t-norms from additive generators, we have to add a $\min$ operation, as the sum $f(a) + f(b)$ may be out of the range of $f$. If there exist $a,b \neq 0$ such that this is the case, we refer to the corresponding t-norm as \textit{nilpotent}, since $a \otimes b = 0$. More accurately, this name arises due to the fact that for such $a \neq 0$, there exists $n \in \N$ such that $\underbrace{a \otimes \cdots \otimes a}_{n \text{ times}} = 0$. 

Examples of nilpotent t-norms are the Łukasiewicz t-norm $\lukand$, and the drastic t-norm $\draand$. In fact, any t-norm whose additive generator is continuous around $0$, and doesnt cover the full possible range $[0, \infty]$, is nilpotent in this manner. Any nilpotent t-norm has a non-negligible region of zero gradient, so this will help inform our future discussion on constructed t-norms.

\subsection{Defuzzification and Interpretation}

Suppose we have embedded a fuzzy LNN layer into our larger neural network. How do we go about interpreting the parameterisation of the function? We have already discussed how we may approach doing so for classical DNFs, but to achieve anything at all, we would need to be able to do so for fuzzy DNFs as well.

We can avoid this issue by ensuring that we learn parameters that are as close to $\B_2$ as possible. To do so, we may consider applying some regularisation term to our loss function, which negatively weights parameter values that are not very crisp. We could then add a regularisation term $\lambda \sum_w v(w)$ over all parameters $w$, and use cross-validation to find an optimal $\lambda$ for learning.

In practice, this hinders learning quite a bit. Much of the strength of fuzzy logic architectures are a direct result of maintaining vagueness - if we are close to values $0$, $1$, it can take a lot of data to modify this.

A more effective approach in practice seems much more naive - that is to simply clamp the value of the parameters $w$ into the range $[0,1]$ at every optimisation step. This allows the model to maintain a level of vagueness where it cannot learn much information, i.e. not ``jumping to conclusions". When it actually can decide the nature of a concept, it can freely do so. We may then choose to terminate our learning when the parameters are sufficiently close to values $\{0,1\}$.

In future sections, we see that using the distance metric $d$ \textit{can} be quite effective in regularisation, but not for directly ensuring crispness - we will use the function $v$ to evaluate the interpretability of our chosen models, but we will not actively optimise for interpretability, we want the models to passively approach crisp parameterisations.

\section{Parameterised Logic}

So far, we have discussed learning the membership of conjunctions and disjunctions only, while keeping the actual definitions of operators constant. We have discussed the fact that some choices of operators may be more successful in learning through backpropagation than others, with the case of the drastic t-norm $\draand$ being a particularly bad choice. How may we find the \textit{most successful} one? 

It's difficult to quantify what this means, so we will not attempt to do so. Instead, we can find some way of defining parameterised \textit{families} of logical systems, and learn the optimals parameters of these families in the same way we learn the parameters of the formulae we are trying to learn.

\subsection{Parameterised T-Norms}

In the previous section, we discussed an easy way to construct new t-norms, so it would make sense to use this technique here. We would like to find a family of t-norms that are all parameterisations of a more general construction. We can begin by parameterising additive generators, and constructing their respective t-norms likewise.

Let $f_p(x) = \frac{1 - x^p}{p}$. This satisfies all the criteria for an additive generator, since $f_p'(x) = -x^{p-1} \leq 0$, and $f_p(1) = 0$. For $p=0$, this is not well defined, but taking the limit as $p$ approaches 0 allows us to extend the function further. In fact this limit is well known - we can say that $f_0(x)=-\log x$, which is actually the additive generator for the product t-norm $\prodand$. What is the resultant t-norm in general?

Since $f^{-1}_p(x) = (1-px)^\frac{1}{p}$, we have $a \otimes_p b = \max\{0, (x^p + y^p - 1)^\frac{1}{p}\}$, and more generally, ${^{\otimes_p}}(\mathbf{x}) = \max\{0, (1 - {^+}((1 - \mathbf{x})^p))^\frac{1}{p}\}$. Of note is that, for $p=1$, this is the Łukasiewicz t-norm $\lukand$. It seems as though many t-norms we have already encountered are members of this family. Indeed, if we further extend the definition we have to $-\infty, \infty$ via continuity, we discover that $\otimes_{-\infty}$ is the minimum t-norm, and $\otimes_\infty$ is the drastic t-norm.

This is wonderful news, as it means with one parametric family of t-norms, we can capture all the examples we might like to study, and then some. These operators are known as the \textit{Schweizer–Sklar t-norms}. Some more parameterised families of t-norms are given by the following additive generators;

$$
\begin{aligned}
\text{(Yager t-norms)}&\ f(x) = (1-x)^p \\
\text{(Aczél–Alsina t-norms)}&\ f(x) = (- \log x)^p \\
\text{(Hamacher t-norms)}&\ f(x) = \log\frac{p+(1-p)x}{x}
\end{aligned}
$$

\subsection{Biased Logic}

There are many properties of classical logical operators that cannot necessarily be captured by fuzzy operators. \textit{Idempotence} is a resultant property of both $\land$ and $\lor$ in boolean algebras, which specifies that for all $a, b \in \B_2$, $(a \land b) \land b = (a \land b)$, and likewise for $\lor$. For most t-norms, this is specifically not the case - an obvious example is the product t-norm, where $abb=ab \iff b = 1$. It would be nice to preserve idempotence, but this is not really required. What if we can sacrifice other properties of boolean operators as well?

We will introduce a new parameterised family of boolean operators, called \textit{weighted non-linear logic} (WNL). WNL is a generalisation of Łukasiewicz logic which introduces a bias term $\beta$, which mirrors the bias used in perceptron layers of classical NN architectures. 

We will define conjunctions and disjunctions like so 



\subsection{Boolean Regularisation}

\subsection{Measuring Similarity}

\section{General Boolean Embeddings}

So far we have met Fuzzy Logic, which fundamentally reinvents the common perceptron architecture seen in most NNs. What if we want to maintain an architecture which is more conventional in it's approach, while still allowing for boolean interpretations? This seems difficult at first - although we can approximate any boolean function, determining the nature of this function is difficult in conventional architectures, as we have seen.

Rather than modifying the architecture of the network, we can instead regularise an existing network architecture to act like a boolean function, while making sure that the regularisation somehow allows us to easily interpret the network's parameters. This sounds rather involved - how might we begin to approach this?

\subsection{Booleans as Vectors}

We have discussed the idea of embedding values representing $\T$ and $\F$ into a continuous space $\B$, and then differentiating over the space in backpropagation. In fuzzy logic, we specifically fix the set $\B_\text{f}\coloneqq[0,1]$ to act as our continuous space, and exploit this by using continuous functions which are guaranteed to behave like logical operators. What if we instead allow for the full range of parameters $\R$, how may we begin to interpret this as a boolean function?

We can take inspiration from other architectures where feature embeddings are common. In natural language processing, word embeddings are maps which take words in a dictionary, and map them into a finite-dimensional vector space. The position the word is mapped to in this vector space can be used to characterise the meaning of the word, and capture relations between words. We could likewise map the boolean values $\T$, $\F$ into a vector space $\B$, and measure the ``crispness" of a learnt function by how similar certain values are to the boolean embeddings in this vector space.

We can define some sort of ``distance metric" over $\B$, a convex, commutative function $d : \B^2 \to \R$ such that $d(\mathbf{a, a}) = 0$ for all $\mathbf{a} \in \B$. This can be a formal distance metric in the topological sense, but it need not be. With this, we can say that the more distant a value $\mathbf{x} \in \B$ is from both $\T$ and $\F$, the less crisp. A particular example which is effective in practice is given later in this section.

\subsection{Learning a Boolean Operator}

Suppose we want to learn the function $\XOR(a, b) = (a \land \lnot b) \lor (\lnot a \land b)$. The model we will use to approximate this will be a NN with a single hidden layer, as it is well known that this cannot be done by a simple single-layer perceptron model. The model will therefore map values between three spaces, which we will call $\B^2 \to X \to \B$, for some $\B, X$, which we will not yet specify.

From here, we can proceed as usual and simply backpropagate with correct input-output pairs to learn an appropriate parameterisation for $\XOR$. However, we can somewhat cheat, because we know the appropriate behaviours of the function we're trying to learn, we only want to learn it over our arbitrary boolean embedding $\B$. We can exploit known properties of $\XOR$ to ``push along" our learning. For instance, we know that;

$$
\begin{aligned}
\text{(Associativity)}&\ \XOR(\XOR(\mathbf{a, b}), \mathbf{c}) = \XOR(\mathbf{a}, \XOR(\mathbf{b, c})) \\
\text{(Commutativity)}&\ \XOR(\mathbf{a, b}) = \XOR(\mathbf{b, a}) \\
\text{(Identity)}&\ \forall \mathbf{a} \in \B, \XOR(\mathbf{a}, \F) = \mathbf{a} \\
\text{(Negation)}&\ \forall \mathbf{a} \in \B, \XOR(\mathbf{a}, \T) = \lnot \mathbf{a}
\end{aligned}
$$

The meaning of $\lnot\mathbf{a}$ here will be explained the next section. We will define a ``$\XOR$-iness loss",
$$\ell_\XOR(\mathbf{x,y}) = d(\XOR(\mathbf{x, y}),\XOR(\mathbf{y, x})) + d(\XOR(\mathbf{x}, \F), \mathbf{x}) + d(\XOR(\mathbf{x}, \T), \lnot \mathbf{x})$$

We can see that a perfect parameterisation of $\XOR$ would minimise this loss. Thus, a complete loss function would look like so,
$$\ell(\mathbf{x,y,z}) = d(\XOR(\mathbf{x,y}),\mathbf{z}) + \lambda_\XOR \ell_\XOR(\mathbf{x,y})$$

The parameters of $\XOR$ are hidden in this representation, as they need not be of any particular architecture.

\subsection{Measuring Similarity}
 
\section{Overview of Different Architectures}