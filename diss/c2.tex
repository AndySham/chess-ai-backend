
\chapter{Logical Neural Networks}
\label{section:lnns}

\section{Introduction}

We will discuss an NN architecture which attempts to learn boolean functions, instead of real functions. The restricted nature of boolean functions, and their use as a mathematical description of logic, allows a intuitive way of representating causation (e.g., if for some object $x$). The models that are created to solve such a problem are known as \textit{Logical Neural Networks}\footnote{also \textit{Neural Logic Networks, Logic Tensor Networks}} (LNNs)\footnote{also NLNs, LTNs}. The subfield of ML concerned with learning solutions to logical problems with gradient-descent based methods is called \textit{Neurosymbolic Machine Learning}.

The goal of the LNNs we discuss here will be to learn the optimal assignments for boolean variables within a statement described by the language of first order logic. This parallels the goal of MLPs to learn the optimal assignments for real variables within linear layers. LNNs achieve this through continuous optimisation using the standard approach of applying SGD methods with backpropagation. 

It is interesting to note that the most widely researched approaches within the field of Artificial Intelligence (AI) until the 1990s were symbolic-based methods. There are many well studied algorithms that, given a set of logical predicates known to be true, attempt to capture the nature of the wider system in a single (ideally simple!) logical statement. These algorithms are known as Inductive Logic Programming (ILP) systems \cite{ilp}. While very useful, they were found to be computationally intractible for more complex problems, notably those in Computer Vision and Natural Language Processing. Famously, Hubert Dreyfus predicted that symbolic methods were inherently incapable of fully capturing the complexity of such problems \cite{symbolicaibad}, stating that these relied primarily on unconscious processes rather than conscious symbolic manipulations. It seems most apt, therefore, to understand the rise of Deep Learning methods in the new millenium as reliant on capturing such ``unconscious'' processes.

The aim of neurosymbolic methods are thus to be able to capture the expressiveness of Deep Learning methods, without sacrificing the interpretability afforded by understanding the model in terms of symbolic manipulations. This is a difficult task! 

\section{Real Logic}

We run into the important issue of $\{\T, \F\}$, the space of boolean values, not being continuous. One way of solving this issue is by extending the domain of possible logical assignments from $\{0,1\}$, as above, to the closed interval $[0,1]$. This is referred to as both \textit{real logic}, (as in \cite{ltn2016}, \cite{analyzefuzzy}, and much of the literature focusing on differentiable applications), and \textit{fuzzy logic} (as in most foundational literature, namely \cite{fuzzysetbook}, \cite{fuzzylogicbook}, \cite{fuzzymetabook}). Using the name ``fuzzy logic'' emphasises the importance of properties which are not relevant to the neurosymbolic architecture we will be discussing here, so we will not be using the term widely, but many of the following constructions are drawn from literature on fuzzy logic. We will discuss approaches in extending important logical operators ($\OR$, $\AND$, $\NOT$, ...) to this new boolean space, and ways we may begin to use it to learn in an interpretable way. 

To fully define a ``real logic", that is, an extension of classical logic to the space $[0,1]$, we want to define all the operators in a manner that, ideally, preserves many of the useful properties we see in the classical definition. At the very least, we want the values over the restricted domain $\{0,1\}$ to be the same as in classical logic.

It is well known that given a finite number of variables $\{x_i\mid i \in 1 , \dots, N \}$, all boolean functions (that is, functions $\phi : \{\T,\F\}^N \rightarrow \{\T,\F\}$) can be expressed in terms of the operators $\lnot$ and $\land$. Explicitly,
$$
\begin{aligned}
    a \lor b &= \lnot (\lnot a \land \lnot b) \\
    a \limply b &= \lnot (a \land \lnot b) \\
    a \lxor b &= \lnot(a \land \lnot b) \land \lnot(\lnot a \land b) \\
    a \lxnor b &= \lnot(a \land b) \land \lnot(\lnot a \land \lnot b) \\
    \forall x, \phi(x) &= \phi(x_1) \land \dots \land \phi(x_N) \\
    \exists x, \phi(x) &= \lnot(\lnot \phi(x_1) \land \dots \land \lnot \phi(x_N)) \\ 
\end{aligned}
$$

If we can extend the operators $\land$ and $\neg$ to the full domain $[0,1]$, we can therefore do so for all the above operators also. For the remainder of this section, we will specify that $\T \coloneqq 1$, and $\F \coloneqq 0$, though there are alternative architectures where this is not the case \cite{embeddedgoogle}.

\subsection{Real Conjunction and Negation}

We will take a set of properties that apply to classical conjunction $\land$ and require that the extension $\land : [0,1]^2 \to [0,1]$ has them also. The properties are;

$$
\begin{aligned}
\text{(Associativity)}&\ (a \land b) \land c = a \land (b \land c) \\
\text{(Commutativity)}&\ a \land b = b \land a \\
\text{(Monotonicity)}&\ a \leq b, c \leq d \implies a \land c \leq b \land d \\
\text{(Identity)}&\ \forall a \in [0,1], a \land 1 = a
\end{aligned}
$$

The above are known as the \textit{T-norm axioms} \cite{tnorms}. Note that they are a strict subset of the axioms required for $([0,1], \land, \lor)$ to be a boolean algebra, namely we are missing distributivity and idempotence. In fact, it is possible to construct a real logic that preserves these properties also, but there is precisely one such logic (known as the Minimum, or Gödel logic), and as we will see, it has flaws that make it less than ideal for gradient descent. The above axioms are, however, enough for $\land$ to have the correct output for the restricted domain $\{0,1\}$. 

We can likewise define negation simply by $\lnot : x \mapsto 1 - x$. In the fuzzy logic literature, this is known as \textit{strong negation}, as there is an alternate formulation of negation that captures the intention of fuzzy logic more effectively.

It can be shown that the axioms as we have defined them allow for an infinite family of possible real logics. Some examples are given below;

\begin{center}
\begin{tabular}{ c | c c }
    Logic & $\forall$ & $\exists$ \\    
    \hline
    Minimum 
    & $\min\{ x_1, \dots, x_N \}$ 
    & $\max\{ x_1, \dots, x_N \}$ \\ 
    Product 
    & $\prod_ix_i$
    & $1 - \prod_{i}(1-x_i)$ \\
    Łukasiewicz
    & $\max\{\sum_ix_i - N + 1, 0\}$ 
    & $\min\{\sum_ix_i, 1\}$ \\
    Schweizer-Sklar
    & $\max\{\sum_ix^p_i - N + 1, 0\}^\frac{1}{p}$ 
    & $\min\{N-\sum_i(1-x_i)^p, 1\}^\frac{1}{p}$ \\
    Yager
    & $\max\{1 - (\sum_i(1-x_i)^p)^\frac{1}{p}, 0\}$ 
    & $\min\{(\sum_ix^p_i)^\frac{1}{p}$, 1\} \\
\end{tabular}
\end{center}
\vspace{5px}
The examples are expressed as conjunctions of $N$ variables, not just 2. $p$ in the table above is a hyperparameter. Figures \ref{fig:conjplots} shows more visually how some of these logics behave. Drastic logic, not listed in the table above, is $0$ whenever 2 or more inputs are less than 1. Otherwise, it agrees with the other logics.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/fuzzy_min_and.png}
        \caption{Minimum}
        \label{fig:minconj}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/fuzzy_prod_and.png}
        \caption{Product}
        \label{fig:prodconj}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/fuzzy_luk_and.png}
        \caption{Łukasiewicz}
        \label{fig:lukconj}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/fuzzy_dra_and.png}
        \caption{Drastic}
        \label{fig:draconj}
    \end{subfigure}
       \caption{Conjunction over various fuzzy logics. Note how pointwise gradients differ.}
       \label{fig:conjplots}
\end{figure}

\section{General Architectures}
\label{section:fuzzyloss}

Now that we have explicit definitions for boolean-like algebras in the domain $[0,1]$, we can begin to formulate how we may learn in this setting.

We first need to consider what problems we may want to solve. A basic problem is that of \textit{satisfiability}, i.e., given a boolean function $\phi : \{0,1\}^K \rightarrow \{0,1\}$ which can be expressed in first-order logic, is there some element $\vx \in \{0,1\}^K$ such that $\phi(\vx) = 1$? If so, we want to find an example of said $\vx$.

We can consider this as a continuous optimisation problem with $\vx$ as a parameter. We can extend $\phi$ to the real domain $[0,1]$ by extending each operator in $\phi$ when expressed in the language of first-order logic. Then, by performing SGD as normal, we aim to minimise the loss function $\ell(\vx) = \lnot\phi(\vx)$.

To match the power of classical MLPs, we also want to be able to learn optimal \textit{functions} $\phi$. Suppose we have a dataset $\{(\vx_i, y_i)\}_{i=1}^N$ where $\forall i, \phi(\vx_i) = y_i$ for some formula $\phi : \{0,1\}^K  \rightarrow \{0,1\}$ which need not have a known representation in classical first-order logic. Further suppose we have a function $\psi : \{0,1\}^{K+P} \rightarrow \{0,1\}$ expressed in first-order logic such that for some $\vw \in \{0,1\}^P$, $\phi(\vx) = \psi(\vx, \vw)$. We consider $\vw$ a parameter here, to be optimised. We could use a variety of losses - in the following we examine $\ell(\vx, y; \vw) = (y \lxor \psi(\vx, \vw))^a$, for some exponent $a \in [1,\infty)$ (as $\bbE_{\vx}[\ell(\vx, \phi(\vx); \vw)] = 0 \iff \psi(\vx, \vw) = \phi(\vx)$), which we refer to as $\XOR$ loss, and binary cross-entropy, as is done in \cite{embeddedlogicreg}. 

It is very feasible to find expressive enough $\psi$, as it can be shown that every boolean function $\phi$ can be expressed in a number of standard forms - Disjunctive Normal Form (DNF), and Conjunctive Normal Form (CNF), being two notable ones. To fit these two representations into the framework we have described, we need to be able to express how every variable $x_i$ is represented in each normal form by appending appropriate parameter variables $w_i$.

In DNF, $\phi$ is expressed as a disjunction of conjunctions, with each conjunction described by a subset of input variables $\subseteq \{x_1,\dots,x_K\}$, each variable being optionally ``signed'' (prepended with $\lnot$). We can capture ``membership'' by boolean variables $m_{ij} \in \{0,1\}$, and signs by $s_{ij} \in \{0,1\}$. This allows us to represent a formula in DNF by

$$
\begin{aligned}
\phi(\vx) &= \exists j,\ 1 \leq j \leq W,\ \phi_j(\vx) \\
\text{where } \phi_j(\vx) &= \forall i,\ m_{ij} \limply (x_i \lxor s_{ij})
\end{aligned}
$$

for some assignment to the $m_{ij}, s_{ij}$. A similar form for CNF is,

$$
\begin{aligned}
\phi(\vx) &= \forall j,\ 1 \leq j \leq W,\ \phi_j(\vx) \\
\text{where } \phi_j(\vx) &= \exists i,\ m_{ij} \land (x_i \lxor s_{ij})
\end{aligned}
$$

In both cases, we can construct a function $\psi$ with the $m_{ij}$ and $s_{ij}$ as parameters, and optimise over these parameters as above. Here $W \in\bbN$ is a hyperparameter which specifies the number of conjunctions (or disjunctions) in the function family $\psi$. Naturally, the larger $W$ is, the more expressive $\psi$ can be, and we know that $W = 2^K$ is enough to capture all possible functions $\phi$. This very closely parallels the Universal Approximation Theorem (UAT) of MLPs with one hidden layer. It also very closely mirrors the concept of ``reducing induction to satisfiability'' seen in many ILP systems \cite{diffilp}.

\section{Analysis of a Toy Problem}

The framework we have given is very general, and does not specify what real logic we are required to use. Indeed it does not even require that every operation need be from the same logic, only that they are correct on classical boolean values $0,1$. It is valuable, therefore, to compare each logic and analyse which logics are useful for which learning tasks.

In \cite{analyzefuzzy}, the problem of satisfiability over $\phi(a,b,c) = (a \land b) \lor (c \land \lnot a)$ is considered. We compare how the parameters $a, b, c$\footnote{In practice, we actually consider satisfiability over parameters $a, b, c \in \bbR$ mapped into $[0,1]$ with a sigmoid function, as we want to converge to parameters in the appropriate bounds.} evolve over each logic, given randomly initialised starting parameters.

Figure \ref{fig:toyconv} show convergence of the output of $\phi$ for 1000 uniformly sampled random initialisations of the parameters $a, b, c$ in the interval $(0,1)$, using Adam optimisation \cite{adam} with a learning rate of $10^{-1}$. Convergence to $1$ represents finding a satisfying assignment, whereas anything else represents a failure to do so.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/toyconv_min.png}
        \caption{Minimum}
        \label{fig:toymin}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/toyconv_prod.png}
        \caption{Product}
        \label{fig:toyprod}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/toyconv_luk.png}
        \caption{Łukasiewicz}
        \label{fig:toyluk}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/toyconv_dra.png}
        \caption{Drastic}
        \label{fig:toydra}
    \end{subfigure}
       \caption{Convergence of toy problem $\phi(a,b,c) = (a \land b) \lor (c \land \lnot a)$ for various fuzzy logics.}
       \label{fig:toyconv}
\end{figure}

We see that the choice of logic has a profound impact on the convergence. Minimum and Product logic both reliably find a satisfying assignment of $\phi$ regardless of initialisation. Łukasiewicz logic finds a satisfying assignment in many instances, but very often fails to optimise at all. Drastic logic (somewhat obviously) never finds a satisfying assignment, regardless of initialisation.

\subsection{Vanishing Gradients}
\label{section:tnormzerograd}

The tests show that a majority of gradient samples for Łukasiewicz logic are precisely $0$. By inspection of Figure \ref{fig:lukconj}, we can see that there are large regions (i.e. regions of non-zero measure) where the function evaluates only to $0$. It is natural that some observations would not allow us to make any meaningful inferences about the optimal values of each variable, especially in cases where the proportion of satisfying inputs is incredibly small. We observe however, that different choices of logic result in different proportions of vanishing gradient observations.

To explain this, we must discuss the nature of conjunction in each logic. In Łukasiewicz logic, for $a, b \in [0,1]$ such that $a + b - 1 < 0$, the gradient of binary conjunction $\land$ is precisely $0$. This means that a full $\frac{1}{2}$ of all possible input arguments (uniformly distributed) give no meaningful inference. Aggregating over $K$ variables, this generalises to a proportion of $1 - \frac{1}{K!}$ inputs having vanishing gradients, which is very obviously not ideal.

Therefore, in this framework, we prefer to use logics such that $\land$ has non-vanishing gradient almost everywhere - these logics are known as \textit{strict}. Minimum and Product logics satisfy this condition, along with certain parameterisations of the Schweizer–Sklar and Yager logics.

\section{Learning Functions}

There are many well known algorithms for efficiently learning classical boolean functions $\phi : \{0,1\}^K \to \{0,1\}$ given prior knowledge of $\phi$ restricting it to a given family. Such algorithms often make logical inferences to determine the proper state of boolean parameters $\vw$, with complexity guarantees that can be described within the Probably Approximately Correct (PAC) learning framework \cite{clt}. We propose equivalent algorithms in differentiable real logic, and hope for results as good, if not better, given an ideal optimisation regime. 

Generalising these methods to real logic would have considerable benefits. For one, given gradient descent is by nature stochastic and (aside from current parameterisation) stateless, these methods would be inherently robust to noisy data and distribution shift in the learning dataset. A possible drawback is that correctness may only be guaranteed for data drawn similarly to the training dataset, introducing a potentially unavoidable bias.

\subsection{Learning Conjunctions}

The problem of learning conjunctions is a classical one in Computational Learning Theory (CLT). It is well known that determining conjunctions can be done PAC-efficiently \cite{clt} and is robust to noisy data \cite{noisyclt}. The goal of this comparison, therefore, is simply to prove the viability of real logic for this application.

The classical algorithm relies on one important observation. Suppose $\phi$ is a conjunction, that is - it is a function of the form 
$$\phi(x_1, \dots, x_D) = y_1 \land \dots \land y_N$$

for $y_i \in \{x_1, \dots, x_D, \lnot x_1, \dots, \lnot x_D\}$. If for some $j$, we have $\lnot x_j$ but $\phi$ returns true, then $x_j$ is not one of the terms $y_i$. Similarly $x_j \land \phi$ implies $\lnot x_j$ is not one of the terms. If we begin with $\phi$ a conjunction over all possible such terms, and remove terms where possible, we are eventually left only with the terms actually present in the conjunction.

As discussed previously, we can model the same thing in real logic by introducing weight and sign parameters $\vm$ and $\vs$, 
$$\psi(\vx; \vm, \vs) = \forall i,\ 1 \leq i \leq D,\ m_i \limply (x_i \lxor s_i)$$

and optimising $\vm$ and $\vs$. In the following, for initial simplicity, we set $\vs = 0$ (i.e. we assume no signs $\lnot$.) 

We aim to learn conjunctions by randomly initialising values $\vm$, before minimising for some loss function, such as those given in Section \ref{section:fuzzyloss}. Data pairs are generated by randomly sampling bits $\{0,1\}^D$, and mapping them through a predefined goal conjunction with $N$ terms. We vary many aspects of this model, including conjunction size $D$, number of present terms $N$, choice of real logic, optimiser, learning rate and loss exponent $a$.

Some results using this architecture are not promising. Figure \ref{fig:conjconvnokeepn} shows convergence of the average distance of a parameter from it's optimal (i.e. correct) value, with the $\XOR$ loss $\ell(\vx, y; \vw) = (\psi(\vx;\vw) \lxor y)^a$ using an Adam optimiser with a learning rate of $10^{-2}$, when learning unsigned conjunctions in the Product logic. Each test is run several times, with the convergence of each run plotted on the graph. Both training and test data are uniformly sampled random bits $\{0,1\}^D$, with each epoch representing 10,000 such samples. We see that for low dimensionality, convergence is quite fast, whereas minimal changes occur with dimensionality any higher than 100. This is of course an issue.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-prod-nokeepn-1t.png}
        \caption{1-term conjunction}
        \label{fig:conjconvnokeepn1}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-prod-nokeepn-5t.png}
        \caption{5-term conjunction}
        \label{fig:conjconvnokeepn5}
    \end{subfigure}
       \caption{Convergence of mean distance from optimal parameters when learning conjunctions in the Product logic. Each test is run 10 times, with $\XOR$ loss, using exponent $a=10$.}
       \label{fig:conjconvnokeepn}
\end{figure}

We see better results when using binary cross-entropy. Figure \ref{fig:conjconvnokeepnbce} displays the results from the same test, with differing loss function. We see that dimensionality $D=100$ now can be learnt, but we still cannot go much higher.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-prod-nokeepn-1t-bce.png}
        \caption{1-term conjunction}
        \label{fig:conjconvnokeepn1bce}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-prod-nokeepn-5t-bce.png}
        \caption{5-term conjunction}
        \label{fig:conjconvnokeepn5bce}
    \end{subfigure}
       \caption{Convergence of mean distance from optimal parameters when learning conjunctions in the Product logic, with exponent binary cross-entropy loss.}
       \label{fig:conjconvnokeepnbce}
\end{figure}

\subsection{Vanishing Gradients in Strict Logics}
\label{section:prodvanishgradient}

To determine why this happens, we need only observe the induced gradient of the network. It is well known that a large family of T-norms can be constructed using what is known as an \textit{additive generator}, that is, a function $f:[0,1]\to\bbR$ associated with a given logic, such that $a \land b = f^{-1}(\min\{f(a) + f(b),f(0)\})$. This definition allows us to also say $\forall i, x_i = f^{-1}(\min\{\sum_i f(x_i),f(0)\})$. Some additive generators have $f(x) \rightarrow \infty$ as $x \rightarrow 0$, and in these cases $a \land b = f^{-1}(f(a) + f(b))$ for $a, b \neq 0$.

Suppose $\sum_i f(x_i) \leq f(0)$ (which is trivially satisfied if $\lim_{x\rightarrow 0}f(x) = \infty$). Then;

$$
\begin{aligned}
\forall i, x_i 
&= f^{-1}(\sum_i f(x_i)) \text{, so} \\
\frac{\partial}{\partial x_j} \forall i, x_i 
&= \frac{
f'(x_j)
}{
f'(f^{-1}(\sum_i f(x_i)))
} \\
&= \frac{
f'(x_j)
}{
f'(\forall i, x_i )
}
\end{aligned}
$$

Some examples of additive generators, as well as explicit values for the derivative above, are given in the following table. Note that if $\sum_i f(x_i) > f(0)$, all gradients are vanishing.

\begin{center}
    \begin{tabular}{ c | c c }
        Logic & $f(x)$ & $\frac{\partial}{\partial x_j} \forall i, x_i$ \\    
        \hline
        Product 
        & $-\log x$
        & $\prod_{i \neq j} x_i$ \\
        Łukasiewicz
        & $1 - x$ 
        & $1$ \\
        Schweizer-Sklar
        & $\frac{1-x^p}{p}$ 
        & $\left(\frac{x_j}{\forall i, x_i}\right)^{p-1}$ \\
        Yager
        & $(1-x)^p$ 
        & $\left(\frac{1-x_j}{1-\forall i, x_i}\right)^{p-1}$ \\
    \end{tabular}
\end{center}

The Minimum logic has no additive generator. From the table, the influence of dimensionality now becomes apparent. 

Suppose each aggregate conjunction $\forall i, x_i$ is over $D$ variables. In the product logic, if the $x_i$ are independently uniformly distributed in $[0,1]$, $\bbE\left[\frac{\partial}{\partial x_j} \forall i, x_i\right] = \bbE\left[\prod_{i \neq j} x_i \right] = 2^{D-1}$. Thus the total derivative $\nabla_\forall$ has expected magnitude $\bbE\left[\lVert \nabla_\forall \rVert_2\right] \leq \bbE\left[\sum_j \frac{\partial}{\partial x_j} \forall i, x_i \right] = D2^{D-1}$. With high dimensionality $D$, the gradients quickly degrade.

Other T-norms do not suffer this same issue. For example, Łukasiewicz logic has a gradient of $1$ in every dimension when the function is non-vanishing, however we have shown previously in Section \ref{section:tnormzerograd} that the proportion of vanishing inputs is it's own curse of dimensionality, as it is not strict. Minimum logic has constant $\lVert \nabla_\forall \rVert_2 = 1$, however we will see later that there are still issues with optimisation. In Appendix \ref{section:ssbounds}, we show that for Schweizer-Sklar logic with parameter $p$, the magnitude of $\nabla_\forall$ is lower bounded by $D^{\frac{1}{p}-\frac{1}{2}}$. In this section, we use $p = -2$, thus making the lower bound $D^{-1} \leq \lVert \nabla_\forall \rVert_2$, which is a vast improvement on an upper bound of $D2^{-D}$ seen in the Product logic. Empirical confirmations are given in Figures \ref{fig:conjgrad10100} and \ref{fig:conjgrad10ss100} in the Appendix.

This can also explain why binary cross-entropy performs better than $\XOR$ loss in this domain. With this loss, outputs are mapped through a $\log$ function. If we consider the Product logic, this is equivalent to interpreting conjunctions as a log summation $\sum_i\log x_i$, which means the values of each variable no longer affect each other's gradients, which would help remedy one cause of the curse of dimensionality. 

\subsection{Architectural Improvements}

To fix the problem described in the previous section, in \cite{analyzefuzzy}, the authors propose a method whereby only a subset of dimensions are conjoined during training passes. Fixing the size of this subset to a number less than 100 should allow for the mitigation of any dimensionality effects on convergence, as we are only optimising this subset of parameters at any one time. This does result in a biased gradient estimator, as we are ignoring many inputs, but in practice this shows to broaden the space of problems that can be solved using this method. This optimisation can be seen as analogous to the common ``dropout" method in traditional neural networks, used to reduce overfitting during training \cite{dropout}. 

We will refer to this method as the \textit{subset optimisation}. Results for conjoining only 50 inputs at a time, chosen at random, are shown in Figure \ref{fig:conjconvkeepn}. We see that this extends the viability of this method to much larger dimensionalities than was previously possible, even when using the Product logic, which we know to not be ideal.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-prod-keep50-1t.png}
        \caption{1-term conjunction}
        \label{fig:conjconvkeepn1}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-prod-keep50-5t.png}
        \caption{5-term conjunction}
        \label{fig:conjconvkeepn5}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-prod-keep50-20t.png}
        \caption{20-term conjunction}
        \label{fig:conjconvkeepn20}
    \end{subfigure}
       \caption{Convergence of mean distance from optimal parameters when learning conjunctions in the Product logic, with the size-50 subset optimisation, under $\XOR$ loss with exponent $a=10$.}
       \label{fig:conjconvkeepn}
\end{figure}

\subsection{Class Imbalance and Parameter Crispness}

In Figure \ref{fig:conjconvkeepn}, we notably see that, whilst applying the subset optimisation makes scaling the dimension of a conjunction viable, what is not viable is increasing the size of the number of terms in the conjunction. If a boolean function is a conjunction of $N$ terms, only a fraction of $2^{-N}$ of all possible inputs will be satisfying. Thus, our learning becomes exponentially worse with increasing $N$. This problem is referred to as class imbalance.

We can explicitly sample the gradients to see how this manifests more directly. Suppose we have a proposed conjunction $\phi(\vx) = x_1 \land x_2 \land x_3$, but we learn that $x_1 = \F$ while $\phi = \T$, for some training data point. If $\phi$ were as we assumed, $\phi$ would be $\F$ in this case, but it is not. Thus $x_1$ cannot be one of the terms in $\phi$. Applying this more generally is what is used in classical algorithms for learning conjunctions given noise-free data \cite{clt}.

We would expect to see a similar effect in the gradient estimator that we derive through backpropagation. To test for this, we uniformly sample random initialisations of a real conjunction model, and observe the gradient estimations that result from an entire epoch of training inputs. Figure \ref{fig:conjgrad1} displays the results of this procedure. A conjunction over $D=10$ variables containing $N=4$ present terms was used to best convey the issue in the empiric results.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_1_falseparam.png}
        \caption{Gradient samples for correct parameter $\F$}
        \label{fig:conjgrad1false}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_1_trueparam.png}
        \caption{Gradient samples for correct parameter $\T$}
        \label{fig:conjgrad1true}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_1_falseparam_avg.png}
        \caption{Mean gradient estimator for correct parameter $\F$}
        \label{fig:conjgrad1falseavg}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_1_trueparam_avg.png}
        \caption{Mean gradient estimator for correct parameter $\T$}
        \label{fig:conjgrad1trueavg}
    \end{subfigure}
       \caption{Gradient estimations over the problem of real conjunctions under $\XOR$ loss with exponent $a=1$.}
       \label{fig:conjgrad1}
\end{figure}

We do see that the gradient estimator behaves differently depending on the true valuation of a given logical parameter. If the true value is $\F$, any input sample with a correct output of $\T$ results in a positive gradient estimation, thus decreasing the parameter on the next optimisation step, as can be seen in Figure \ref{fig:conjgrad1false}. This is exactly the kind of behaviour we would expect, and it represents a faithful translation of logical inference to the real domain. Whenever the output is $\F$, the model increases all parameters, as no meaningful inference can be made, which can be seen in both Figures \ref{fig:conjgrad1false} and \ref{fig:conjgrad1true}. This can be interpreted as the model aiming to be robust to noisy data. 

The samples above show that it is impossible for terms which are indeed part of the true conjunction to not be represented in the learnt model, as their parameterisations can only increase. The mean gradient estimator in Figure \ref{fig:conjgrad1trueavg} reflects this, as it is firmly below 0 no matter the parameterisation. If the term is not in the true conjunction, however, Figure \ref{fig:conjgrad1falseavg} shows that we unfortunately see that the gradient estimator is still negative, meaning the term appears in the learnt conjunction, regardless of initial parameterisation. 

This is the result of the class imbalance problem. Despite the gradients being in the right direction at every step, the overall result is inaccurate, because inferences are not being well capitalised if they cannot be frequently made.

What is interesting is that increasing the value of the exponent $a$ appears to resolve this issue. In all our successful tests, we use a value of $a=10$. A plot of gradient samples can be found in Appendix \ref{fig:conjgrad10}. We see that as the exponent increases, the direction of the gradients become more accurate, and the magnitude of the gradients become larger for significantly incorrect values. However, for parameter values that are near to $0.5$, no significant gradient is observed. Values 0 and 1 are referred to as ``crisp'' boolean values, so one could interpret this as the regime failing to ``crispen'' values effectively, despite correctly signed gradients. Appendix \ref{section:moregradients} contains further examples of how this loss fails due to the curse of dimensionality - in all the plots in this section, we use dimensionality $D=10$, but for $D=100$ the gradients quickly vanish.

A significant observation therefore, which was eluded to Section \ref{section:prodvanishgradient}, is that binary cross-entropy, with appropriate choice of logic, suffers none of these problems. 


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_bce_falseparam_10dim.png}
        \caption{Gradient samples for correct parameter $\F$}
        \label{fig:conjgrad1falsebce}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_bce_trueparam_10dim.png}
        \caption{Gradient samples for correct parameter $\T$}
        \label{fig:conjgrad1truebce}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_bce_falseparam_10dim_avg.png}
        \caption{Mean gradient estimator for correct parameter $\F$}
        \label{fig:conjgrad1falseavgbce}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/grad_prod_bce_trueparam_10dim_avg.png}
        \caption{Mean gradient estimator for correct parameter $\T$}
        \label{fig:conjgrad1trueavgbce}
    \end{subfigure}
       \caption{Gradient estimations over the problem of real conjunctions under binary cross-entropy loss.}
       \label{fig:conjgrad1bce}
\end{figure}

Figure \ref{fig:conjgrad1bce} shows how binary-cross entropy loss not only has a gradient estimator with correct direction, but also of greater magnitude, regardless of current parameterisation. Later empirical results will show that this does have a profound effect on convergence.

We have demonstrated that varying our choice of loss has a significant effect on not only the rate of convergence, but the correctness of the gradient estimator as well. Appendix \ref{section:moregradients} contains more such examples, that show how varying the logic, dimension and choice of loss function have an effect on the gradient estimator.

\subsection{Empiric Comparison of Logics}

We have discussed what effect varying the choice of logic has on the gradient estimator $\nabla_\forall$, but we have not seen what effect this directly has on convergence. Figure \ref{fig:conjconvlogics} shows that this effect is profound. Here, a ``correct'' parameter is one which is within $0.5$ of it's optimal value.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-cp-100dim-keep50-5t.png}
        \caption{Proportion of correct parameters for $D=100$}
        \label{fig:conjconvcpd100}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-100dim-keep50-5t.png}
        \caption{Mean distance of parameters from optimum for ${D=100}$}
        \label{fig:conjconvpmdd100}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-cp-10000dim-keep50-5t.png}
        \caption{Proportion of correct parameters for $D=10000$}
        \label{fig:conjconvcpd10000}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-10000dim-keep50-5t.png}
        \caption{Mean distance of parameters from optimum for ${D=10000}$}
        \label{fig:conjconvpmdd10000}
    \end{subfigure}
       \caption{Convergence over various logics under $\XOR$ loss $a=10$, $N=5$ terms, subset optimisation.}
       \label{fig:conjconvlogics}
\end{figure}

\begin{wrapfigure}[12]{r}{0.45\linewidth}
    \centering
    \vspace{-35.0pt}
    \includegraphics[width=0.30\textwidth, height=0.30\textwidth]{imgs/fuzzy_ss-2_and.png}
    \vspace{-10.0pt}
    \caption[width=0.3\textwidth]{Schweizer-Sklar T-norm ($b=-2$)}
    \label{fig:ssand}
\end{wrapfigure}

The plots show that, as in Figure \ref{fig:toyconv}, Lukasiewicz logic fails even with the discussed optimisations. 

Minimum logic is not far behind - while Figure \ref{fig:conjconvcpd100} shows the proportion of correct parameters increasing gradually for $D=100$, this proportion does not improve whatsoever for $D=10000$. This may be due to the fact that in Minimum logic, only one input variable to a conjunction has non-zero gradient at a time, as can be seen in Figure \ref{fig:minconj}. This restriction does not allow the optimisation regime to traverse the parameter space in an efficient manner. \cite{analyzefuzzy} refers to this problem as the \textit{binarization} of gradients. Empirical examples of this binarization can be found in Appendix \ref{fig:conjgrad10m}.

We have already mentioned in Section \ref{section:prodvanishgradient} that Product logic suffers it's own curse of dimensionality, and we can see this in how it initially has poor convergence in Figure \ref{fig:conjconvcpd10000}. However, as parameters are optimised, the all inputs to the conjunction approach 1, and thus the gradients increase in value. This results in a quickening in convergence rate around 40 epochs, before finally plateauing.

We also see the Schweizer-Sklar logic performs very efficiently, as the bounds derived in Appendix \ref{section:ssbounds} predict. What is interesting to note is that, for $p=0$, Schweizer-Sklar logic is precisely the Product logic, and for $p=-\infty$, it is precisely the Minimum logic. We can therefore also interpret Schweizer-Sklar (with a parameter $p=-2$ as used in the tests) as being a ``middle ground'' between the two logics, having a guaranteed significant gradient without suffering from binarization. 

Of note is that, as we can see in Figure \ref{fig:conjconvpmdd10000}, Product logic still outperforms Schweizer-Sklar logic in the ``crispness'' of it's learnt real parameters, as the former converges to an average distance considerably lower than that of the latter. Under binary cross-entropy loss, we see an interesting result. 

Figure \ref{fig:conjconvlogicsbce} shows the result of training under binary cross-entropy loss, both with and without the subset optimisation. The subset optimisation continues to work with the Product logic, albeit not as well. It however has a disasterous effect on Schweizer-Sklar logic, which not only fails to converge to optimal values, but goes in the wrong direction.

This is not observed without the subset optimisation. As we'd expect, the curse of dimensionality results in the Product logic implementation failing to converge at all. The Schweizer-Sklar implementation performs spectacularly well. It is significantly more efficient than under $\XOR$ loss - under $\XOR$ loss, with a dimensionality $D=10000$, it takes roughly 40 epochs to achieve parameter correctness greater than 90\%. Under binary cross-entropy loss, it only takes around 5. It further improves in the crispness of it's learnt parameters. In Figure \ref{fig:conjconvlogics}, we saw that every logic failed to achieve optimal (i.e. close to 0) distance from optimal parameters. Under this regime, however, we can achieve this. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-cp-10000dim-keep50-5t-bce.png}
        \caption{Proportion of correct parameters with subset optimisation.}
        \label{fig:conjconvcpbceso}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-10000dim-keep50-5t-bce.png}
        \caption{Mean distance of parameters from optimum with subset optimisation.}
        \label{fig:conjconvpmdbceso}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-cp-10000dim-5t-bce.png}
        \caption{Proportion of correct parameters without subset optimisation.}
        \label{fig:conjconvcpbce}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-pmd-10000dim-5t-bce.png}
        \caption{Mean distance of parameters from optimum without subset optimisation.}
        \label{fig:conjconvpmdbce}
    \end{subfigure}
       \caption{Convergence over various logics under binary cross-entropy loss, $N=5$ terms, $D=10000$ dimensionality.}
       \label{fig:conjconvlogicsbce}
\end{figure}

\subsection{Suboptimal Predictions}

Crispness is important if we wish to use any of these models to actually predict outputs. Suppose that we have correct parameterisations for every term, but with a distance of 0.25 from crisp values $0, 1$. In product logic, $0.25 \limply 0 = 0.75$, whereas in classical logic this would round to $\F \limply \F = \T$. taking the product over a number of such terms that increases linearly with $N$ gives us an upper bound of $O(0.75^N)$ for the actual output. Since randomly sampled input bits generally have a proportion of bits with value $0$ close to $\frac{N}{2}$, most outputs are guaranteed to be near $0$. While many regimes can guarantee 100\% of parameters are ``correct'', the nature of fuzzy logic means the output isn't necessarily translated into correct outputs.

Figure \ref{fig:conjconvco} shows the convergence of the proportion of corrects outputs under various regimes. Of note is that, for $N \geq 100$ under Product logic, the proportion of correct outputs does not meaningfully differ through optimisation. The stagnant proportion is also somewhat around $1 - 2^{-5} \approx 0.968$, which suggests that the output only ever evaluates to false, as this $2^{-5}$ represents the proportion of true outputs for conjunctions of 5 terms. 

This is not surprising, as we have just discussed how this model poorly approximates the true crisp parameterisations $0,1$, which would result in the output degrading to $0$, even if the parameters are all correct.

However, as we have discussed, Schweizer-Sklar logic can approximate crisp values, and does not suffer the curse of dimensionality (as much). This can be observed in Figure \ref{fig:conjconvcoss}. In fact, convergence seems to suffer only linearly in $\log D$.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-co-prod-keep50-5t.png}
        \caption{Product logic, subset optimisation, $\XOR$, $a=10$ loss.}
        \label{fig:conjconvcopxor}
    \end{subfigure}
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-co-prod-keep50-5t-bce.png}
        \caption{Product logic, subset optimisation, binary cross-entropy loss.}
        \label{fig:conjconvcopbce}
    \end{subfigure}
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/conj-co-ss-nokeepn-5t-bce.png}
        \caption{Schweizer-Sklar logic $p=-2$, no subset optimisation, binary cross-entropy loss.}
        \label{fig:conjconvcoss}
    \end{subfigure}
       \caption{Convergence of proportion of correct outputs, $N=5$ terms.}
       \label{fig:conjconvco}
\end{figure}

\subsection{Conclusions}

In learning conjunctions, we have experimented with a variety of logics, loss functions, and additional optimisations to find solutions to the many issues experienced in relaxing classical logic to the real domain. Class imbalance results in inaccurate gradient estimators, while the curse of dimensionality results in vanishing ones. Many logics have gradient estimators which are precisely 0 in a non-negligible region of the parameter space. Even in logics where this isn't the case, for example the Minimum logic, we still suffer due to the gradients being \textit{binarized}, we can only optimise one input at a time. Finally, the lack of crispness in our learnt parameters prevents the predicted outputs from being accurate.

If we learn this problem using Schweizer-Sklar logic under a binary cross-entropy loss, we can resolve all these issues to a satisfactory level. Formal lower bounds exist on the magnitude of the gradient estimator for increasing dimensionality, and class imbalance does not as significantly effect the correctness of the gradient direction under this loss. We can now experiment with more complicated problems. 

\pagebreak

\section{Learning Arbitrary Functions}
\label{section:realdnfs}

An important result from CLT is that with the assumption that $\textsc{RP} \neq \textsc{NP}$, it can be shown that learning boolean functions in DNF is not PAC-efficient \textit{in general} \cite{clt}. Hopefully, relaxing boolean constraints from the discrete to the real domain would allow us to overcome this issue.

\todo{Add test results here}

\todo{Add analysis}

It is important to note that 

- Suddenly not convex (much like MLP)
- Suffers greatly


\section{Alternative Frameworks}

The toy problems above show that the framework we have introduced is not adept at learning boolean functions in general. The appeal instead is that it is simple, and \textit{extensible}. A neurosymbolic layer as we have described can be embedded within a larger model, that may also contain traditional perceptron layers. These mixed models could lend their correctness to traditional NN theory, and their interpretability to their neurosymbolic aspect. In this section, we will discuss other existing neurosymbolic methods, and judge them based on the possibility of use in this application.

One common feature of many of the architectures we have not yet discussed is that logical variables $\vx$ explicitly describe some parent object. In the language of our real logic architecture, there exists some universe of objects $O$ such that boolean inputs to $\phi$ describe an object $o \in O$ in the form of a \textit{unary atom}, e.g. $\textsc{isGreen}(o), \textsc{isBig}(o)$. The output of $\phi$ can be interpreted as the valuation of a further unary predicate. Learning $\phi$ is equivalent to learning some ``if and only if" relationship between different properties of the object $o\in O$, if such a relationship exists. 

\subsection{Alternative Applications of Real Logic}

As mentioned previously, classical symbolic approaches to AI research involve the use of ILP systems. A formal description of an ILP system is one which can solve problems of the following form. Given some background knowledge $B$ of the world of objects $O$ in the form of a logical statement, and new observations of \textit{positive and negative examples} $E^+$ and $E^-$, expressed as conjunctions of \textit{ground literals} (e.g. $\textsc{isGreen}(o), \lnot \textsc{areFriends}(a,b)$), we aim to find some hypothesis statement $h$ such that $B \land h$ satisfies all new observations. Formally, we require \textit{sufficiency} $B \land h \lentails E^+$, and \textit{consistency} $B \land h \land E^- \lnentails \F$ of the constructed $h$.

Our current real logic framework solves a real relaxation of the ILP problem where $E^+$ and $E^-$ are all instances of the same unary predicate. There exist differentiable real logic solvers for general ILP problems \cite{diffilp}, \cite{diffilpGT}. Such implementations improve on classical ILP methods as they are robust to noisy data while still being adept at pattern finding.

Other implementations relax real logic further by removing the requirement for all T-norm axioms to be satisfied. \cite{lnnibm} introduces \textit{Weighted Non-Linear Logic}, which is conceptually similar to Łukasiewicz logic with the addition of a bias parameter $\beta$ which is also optimised during learning. The source paper promotes this reformulation as it removes constraints from the problem of optimisation, though the algorithm introduced in the paper as stated cannot be embedded into a mixed model.

\subsection{Embedded Logic and Relations}

Many developments is neurosymbolic learning involve embedding objects into $n$-dimensional real vector spaces to exploit properties of this space. Word2vec \cite{word2vec} is a model for learning optimal embeddings of natural language atoms in a high-dimensional vector space. Such embeddings were found to preserve relationships between words through vector arithmetic \cite{word2vecrelations} (e.g. Father $-$ Man $+$ Woman $\approx$ Mother). This can be interpreted as a (relaxed) ILP system which learns over the space of \textit{binary} atoms. Learning embeddings of objects $o \in O$ in this manner will aid in extending our current real logic model to mixed models.

In \cite{ltn2016}, the notion of a \textit{grounding} is introduced - before applying any real logic operators, objects $o \in O$ are mapped into $\R^n$ through some canonical function $\calG : O \to \R^n$. An $m$-ary predicate $P$ is then interpreted as a function $\R^{n\times m} \to [0,1]$. The value of $\calG$ may then be learnt in a manner which best evaluates predicates $P$ for elements of the test dataset. 

Further, \cite{embeddedgoogle} suggests that not only should objects be embedded into a real vector space, but that boolean values should be as well. In this system, boolean values are encoded over an $n$-dimensional unit sphere $S_{n-1}$, and logical operators are required to map the encodings of $\T$ and $\F$ appropriately in this space. Introducing new dimensions may aid in improving model quality, without sacrificing interpretability as activations can be collapsed back into ``real logic'' through a similarity metric $\textsc{Sim}:S_{n-1}^2 \to [0,1]$. Cosine similarity (which corresponds to minimum distance travelling along the unit sphere) is generally used. \cite{embeddedlogicreg} improves upon this by also learning core operators $\land, \lor, \lnot$ as DNNs, with correctness ensured through regularisation rather than the architecture itself. This model has been used to develop high quality collaborative filtering algorithms \cite{embeddedcollabreasoning}.

\subsection{Bayesian Models and Probabilistic Logic}

Another approach which could be considered a continuous relaxation of classical logic is in modelling boolean values as distributions over deterministic values $\{\T, \F\}$. Bayesian methods are well studied for this application, with many efficient methods existing for learning the distributions of random variables expressed in terms of a random field. Probabilistic programming languages \cite{gordon2014probabilistic} are able to describe such problems over Bayesian Networks in an highly interpretable manner. Learning probability distributions over the parameter space $\vw$ may prove a more interpretable solution, and further may solve some of the convergence issues seen due to many problems being non-convex. If we interpret real booleans $[0,1]$ as parameters of a Bernoulli distribution over values $\{\T, \F\}$, we can analogize real logic models using the product logic to modelling a random field such that all parameters $\vw$ are independent of each other. In this interpretation, we see that using general Bayesian models increases the expressivity of our model by introducing dependencies between parameters $\vw$. Arbitrary dependencies can be modelled using Normalizing Flows \cite{normalizingflows}.

Bayesian methods are very general, and many approaches exist which are specialised for uses in logic. Bayesian networks are restrictive as they model variable dependencies as a Directed Acyclic Graph (DAG), whereas many dependencies in logic are cyclic (most notably the relation $\iff$). Markov Networks describe dependencies between random variables as edges in a graph, meaning that any two variables that are not adjacent are conditionally independent. These are used to capture logical inferences in the popular Markov Logic Network (MLN) model \cite{markovlogicnetworks}, which corresponds logical atoms to vertices, and formulae to cliques in the graph. Probabilistic Soft Logic \cite{probsoftlogic} further develops on this by introducing a PPL to describe instances of a model similar to MLNs, which can be used to better interpret the results of learning using such methods.


