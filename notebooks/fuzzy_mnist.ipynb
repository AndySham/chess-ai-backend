{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36723e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src')) # include top level package in python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f826561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".container { \n",
       "    width: 100% !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".container { \n",
    "    width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e40f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from model.fuzzy_layer import (\n",
    "    FuzzySignedConjunction, \n",
    "    FuzzySignedDisjunction, \n",
    "    FuzzyUnsignedConjunction, \n",
    "    FuzzyUnsignedDisjunction, \n",
    "    FuzzyNumKeepup, \n",
    "    keepidx, \n",
    "    fuzzy_dropup\n",
    ")\n",
    "from model.fuzzy_logic import ProductLogic, MinimumLogic, LukasiewiczLogic, DrasticLogic, SchweizerSklarLogic\n",
    "from model.fuzzy_layer import FuzzyParam\n",
    "from model.bool_logic import BoolLogic\n",
    "from cache import TestMetric, TrainingRegime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from util import shuffle\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "logic = BoolLogic()\n",
    "flogic = ProductLogic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ef9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "trainset = datasets.MNIST(root='./_mnist', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./_mnist', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32)\n",
    "testloader = DataLoader(testset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c1d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectOutputMetric(TestMetric):\n",
    "    def name(self):\n",
    "        return \"correct-output\"\n",
    "    \n",
    "    def measure_model(self, model, it):\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in it():\n",
    "                preds = model(imgs.to(device))\n",
    "                if hasattr(model.__class__, \"zero_reg\"):\n",
    "                    model.zero_reg()\n",
    "                pred_labels = preds.argmax(dim=-1)\n",
    "                correct_count += (pred_labels == labels.to(device)).sum()\n",
    "                total_count += labels.numel()\n",
    "\n",
    "        return (correct_count / total_count).item()\n",
    "    \n",
    "class LossMetric(TestMetric):\n",
    "    def name(self):\n",
    "        return \"test-loss\"\n",
    "\n",
    "    def measure_model(self, model, it):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in it():\n",
    "                preds = model(imgs.to(device))\n",
    "                if hasattr(model.__class__, \"zero_reg\"):\n",
    "                    model.zero_reg()\n",
    "                loss = loss_fn(preds, labels.to(device))\n",
    "                total_loss += loss\n",
    "                total_count += labels.numel()\n",
    "\n",
    "        return (total_loss / total_count).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1cdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedRegMetric(TestMetric):\n",
    "    def name(self):\n",
    "        return \"embed-reg\"\n",
    "    \n",
    "    def measure_model(self, model, it):\n",
    "        total_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in it():\n",
    "                model(imgs.to(device))\n",
    "                total_count += imgs.size(0)\n",
    "\n",
    "        reg = model.logic_reg()\n",
    "        model.zero_reg()\n",
    "            \n",
    "        return (reg / total_count).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254c31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMNIST(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0052a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyMNIST(nn.Module):\n",
    "    \n",
    "    def __init__(self, logic, lin_channels, logic_layers=1, logic_width=64, keepn=None):\n",
    "        super().__init__()\n",
    "        self.lin_channels = lin_channels\n",
    "        self.logic_width = logic_width\n",
    "        self.logic_layers = logic_layers\n",
    "        self.keepn = keepn\n",
    "        \n",
    "        fuzzy_intermed_layers = [(\n",
    "            FuzzySignedDisjunction(logic_width, logic_width, logic=logic, keepn=keepn),\n",
    "            FuzzySignedConjunction(logic_width, logic_width, logic=logic, keepn=keepn)\n",
    "        ) for i in range(1, logic_layers)] \n",
    "        fuzzy_intermed_layers = [layer for tup in fuzzy_intermed_layers for layer in tup]\n",
    "        \n",
    "        lin = nn.Linear(28*28, self.lin_channels)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            lin,\n",
    "            nn.Sigmoid(),\n",
    "            FuzzySignedConjunction(self.lin_channels, logic_width, logic=logic, keepn=keepn),\n",
    "            *fuzzy_intermed_layers,\n",
    "            FuzzySignedDisjunction(logic_width, 10, logic=logic, keepn=keepn),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(lin.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7206aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.embed_layer import EmbedDNF, EmbedEncode, EmbedDecode\n",
    "from model.embed_logic import EmbedLogic\n",
    "\n",
    "class EmbedMNIST(nn.Module):\n",
    "    \n",
    "    def __init__(self, lin_width, logic_layers=1, logic_width=64, embed_dims=3, reg=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.logic = EmbedLogic(embed_dims, calculate_reg=reg)\n",
    "        self.lin_width = lin_width\n",
    "        self.logic_width = logic_width\n",
    "        self.logic_layers = logic_layers\n",
    "        \n",
    "        if logic_layers == 1:\n",
    "            embed_widths = [(self.lin_width, logic_width, 10)]\n",
    "        else:\n",
    "            embed_widths = [\n",
    "                (self.lin_width, logic_width, logic_width),\n",
    "                *((logic_width, logic_width, logic_width) for _ in range(logic_layers - 2)),\n",
    "                (logic_width, logic_width, 10),\n",
    "            ]\n",
    "            \n",
    "        embed_layers = [EmbedDNF(shape=shape, logic=self.logic) for shape in embed_widths]\n",
    "        \n",
    "        self.lin_model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, self.lin_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.lin_width, self.lin_width * embed_dims),\n",
    "        )\n",
    "            \n",
    "        self.embed_model = nn.Sequential(\n",
    "            EmbedEncode(embed_dims=embed_dims),\n",
    "            *embed_layers,\n",
    "            EmbedDecode(logic=self.logic),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "            \n",
    "    def forward(self, input):\n",
    "        dims = self.logic.embed_dims\n",
    "        a_0 = self.lin_model(input)\n",
    "        z_0 = a_0.reshape(*a_0.shape[:-1], a_0.size(-1) // dims, dims)\n",
    "        return self.embed_model(z_0)\n",
    "    \n",
    "    def logic_reg(self):\n",
    "        return self.logic.logic_reg()\n",
    "    \n",
    "    def zero_reg(self):\n",
    "        return self.logic.zero_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2abb0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTRegime(TrainingRegime):\n",
    "    def __init__(self, lr=1e-2, no_runs=1):\n",
    "        super().__init__(\"./mnist/\", no_runs)\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        self.tests = [\n",
    "            CorrectOutputMetric(),\n",
    "            LossMetric(),\n",
    "        ]\n",
    "        \n",
    "        self.trainloader = DataLoader(trainset, batch_size=128)\n",
    "        self.testloader = DataLoader(testset, batch_size=128)\n",
    "        \n",
    "        self.optims = [None] * no_runs\n",
    "        \n",
    "    def get_optim(self, run_no):\n",
    "        optim = self.optims[run_no - 1]\n",
    "        if optim is None:\n",
    "            model = self.get_loaded_model(run_no)\n",
    "            optim = self.optims[run_no - 1] = (\n",
    "                torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "            )\n",
    "        return optim\n",
    "        \n",
    "    def regime_str(self):\n",
    "        return (\n",
    "            \"MNIST | ADAM, LR = %s | STANDARD MODEL\"\n",
    "            % ( \n",
    "                self.lr\n",
    "                \n",
    "            )\n",
    "        ) \n",
    "        \n",
    "    def regime_filename_elems(self):\n",
    "        elems = [\n",
    "            \"mnist\",\n",
    "            str(self.lr),\n",
    "            \"standard\",\n",
    "        ]\n",
    "        return elems\n",
    "        \n",
    "    def training_dataloader(self, run_no):\n",
    "        return self.trainloader\n",
    "    \n",
    "    def testing_dataloader(self, run_no):\n",
    "        return self.testloader\n",
    "    \n",
    "    def training_step(self, run_no, model):\n",
    "        optim = self.get_optim(run_no)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        def step(tup):\n",
    "            imgs, labels = tup\n",
    "            preds = model(imgs.to(device))\n",
    "            loss = loss_fn(preds, labels.to(device))\n",
    "            \n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            \n",
    "            grads = torch.tensor(0.0)\n",
    "            for param in model.parameters():\n",
    "                with torch.no_grad():\n",
    "                    print(param.grad.abs().sum().cpu(), param.grad.shape)\n",
    "            raise Error\n",
    "            \n",
    "            optim.step()\n",
    "            \n",
    "        return step\n",
    "        \n",
    "    def new_model(self):\n",
    "        return ModelMNIST().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea5ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTRealRegime(TrainingRegime):\n",
    "    def __init__(self, logic, lin_channels=16, logic_width=64, logic_layers=1, keepn=32, lr=1e-2, exp=1, no_runs=1):\n",
    "        super().__init__(\"./mnist/\", no_runs)\n",
    "        \n",
    "        self.flogic = logic\n",
    "        self.lin_channels = lin_channels\n",
    "        self.logic_width = logic_width\n",
    "        self.logic_layers = logic_layers\n",
    "        self.keepn = keepn\n",
    "        self.lr = lr\n",
    "        self.exp = exp\n",
    "        \n",
    "        self.tests = [\n",
    "            CorrectOutputMetric(),\n",
    "            LossMetric(),\n",
    "        ]\n",
    "        \n",
    "        self.trainloader = DataLoader(trainset, batch_size=32)\n",
    "        self.testloader = DataLoader(testset, batch_size=32)\n",
    "        \n",
    "        self.optims = [None] * no_runs\n",
    "        \n",
    "    def get_logic_str(self):\n",
    "        if isinstance(self.flogic, ProductLogic):\n",
    "            return \"product\"\n",
    "        elif isinstance(self.flogic, MinimumLogic):\n",
    "            return \"minimum\"\n",
    "        elif isinstance(self.flogic, LukasiewiczLogic):\n",
    "            return \"lukasiewicz\"\n",
    "        elif isinstance(self.flogic, DrasticLogic):\n",
    "            return \"drastic\"\n",
    "        elif isinstance(self.flogic, SchweizerSklarLogic):\n",
    "            return \"schweizer-sklar\"\n",
    "        else:\n",
    "            return \"fuzzy\"\n",
    "        \n",
    "    def get_optim(self, run_no):\n",
    "        optim = self.optims[run_no - 1]\n",
    "        if optim is None:\n",
    "            model = self.get_loaded_model(run_no)\n",
    "            optim = self.optims[run_no - 1] = (\n",
    "                torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "            )\n",
    "        return optim\n",
    "        \n",
    "    def regime_str(self):\n",
    "        keep_str = \"\" if self.keepn is None else \" | KEEP N = %s\" % self.keepn\n",
    "        return (\n",
    "            \"MNIST | ADAM, LR = %s | %s LOGIC | %s LINEAR CHANNELS | WIDTH %s LOGIC LAYER | %s LOGIC LAYERS | EXP %s%s\"\n",
    "            % ( \n",
    "                self.lr,\n",
    "                self.get_logic_str().upper(), \n",
    "                self.lin_channels, \n",
    "                self.logic_width, \n",
    "                self.logic_layers, \n",
    "                self.exp,\n",
    "                keep_str\n",
    "                \n",
    "            )\n",
    "        ) \n",
    "        \n",
    "    def regime_filename_elems(self):\n",
    "        elems = [\n",
    "            \"mnist\",\n",
    "            str(self.lr),\n",
    "            self.get_logic_str(),\n",
    "            \"%slc\" % self.lin_channels,\n",
    "            \"%sw\" % self.logic_width,\n",
    "            \"%slyrs\" % self.logic_layers\n",
    "        ]\n",
    "        if self.exp != 1:\n",
    "            elems.append(\"exp%s\" % self.exp)\n",
    "        if self.keepn is not None:\n",
    "            elems.append(\"keep%s\" % self.keepn)\n",
    "        return elems\n",
    "        \n",
    "    def training_dataloader(self, run_no):\n",
    "        return self.trainloader\n",
    "    \n",
    "    def testing_dataloader(self, run_no):\n",
    "        return self.testloader\n",
    "    \n",
    "    def training_step(self, run_no, model):\n",
    "        optim = self.get_optim(run_no)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        def step(tup):\n",
    "            imgs, labels = tup\n",
    "            preds = model(imgs.to(device))\n",
    "            loss = loss_fn(preds, labels.to(device)) ** self.exp\n",
    "            #losses.append(loss.item())\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        return step\n",
    "        \n",
    "    def new_model(self):\n",
    "        return EmbedMNIST(\n",
    "            logic=self.flogic, \n",
    "            lin_channels=self.lin_channels, \n",
    "            logic_layers=self.logic_layers, \n",
    "            logic_width=self.logic_width,\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e24eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTRealMLPRegime(TrainingRegime):\n",
    "    def __init__(self, logic, lin_channels=16, logic_width=64, logic_layers=1, keepn=32, lr=1e-2, exp=1, no_runs=1):\n",
    "        super().__init__(\"./mnist/\", no_runs)\n",
    "        \n",
    "        self.flogic = logic\n",
    "        self.lin_channels = lin_channels\n",
    "        self.logic_width = logic_width\n",
    "        self.logic_layers = logic_layers\n",
    "        self.keepn = keepn\n",
    "        self.lr = lr\n",
    "        self.exp = exp\n",
    "        \n",
    "        self.tests = [\n",
    "            CorrectOutputMetric(),\n",
    "            LossMetric(),\n",
    "        ]\n",
    "        \n",
    "        self.trainloader = DataLoader(trainset, batch_size=32)\n",
    "        self.testloader = DataLoader(testset, batch_size=32)\n",
    "        \n",
    "        self.optims = [None] * no_runs\n",
    "        \n",
    "    def get_logic_str(self):\n",
    "        if isinstance(self.flogic, ProductLogic):\n",
    "            return \"product\"\n",
    "        elif isinstance(self.flogic, MinimumLogic):\n",
    "            return \"minimum\"\n",
    "        elif isinstance(self.flogic, LukasiewiczLogic):\n",
    "            return \"lukasiewicz\"\n",
    "        elif isinstance(self.flogic, DrasticLogic):\n",
    "            return \"drastic\"\n",
    "        elif isinstance(self.flogic, SchweizerSklarLogic):\n",
    "            return \"schweizer-sklar\"\n",
    "        else:\n",
    "            return \"fuzzy\"\n",
    "        \n",
    "    def get_optim(self, run_no):\n",
    "        optim = self.optims[run_no - 1]\n",
    "        if optim is None:\n",
    "            model = self.get_loaded_model(run_no)\n",
    "            optim = self.optims[run_no - 1] = (\n",
    "                torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "            )\n",
    "        return optim\n",
    "        \n",
    "    def regime_str(self):\n",
    "        keep_str = \"\" if self.keepn is None else \" | KEEP N = %s\" % self.keepn\n",
    "        return (\n",
    "            \"MNIST | ADAM, LR = %s | %s LOGIC | %s LINEAR CHANNELS | WIDTH %s LOGIC LAYER | %s LOGIC LAYERS | EXP %s%s\"\n",
    "            % ( \n",
    "                self.lr,\n",
    "                self.get_logic_str().upper(), \n",
    "                self.lin_channels, \n",
    "                self.logic_width, \n",
    "                self.logic_layers, \n",
    "                self.exp,\n",
    "                keep_str\n",
    "                \n",
    "            )\n",
    "        ) \n",
    "        \n",
    "    def regime_filename_elems(self):\n",
    "        elems = [\n",
    "            \"mnist\",\n",
    "            str(self.lr),\n",
    "            self.get_logic_str(),\n",
    "            \"%slc\" % self.lin_channels,\n",
    "            \"%sw\" % self.logic_width,\n",
    "            \"%slyrs\" % self.logic_layers\n",
    "        ]\n",
    "        if self.exp != 1:\n",
    "            elems.append(\"exp%s\" % self.exp)\n",
    "        if self.keepn is not None:\n",
    "            elems.append(\"keep%s\" % self.keepn)\n",
    "        return elems\n",
    "        \n",
    "    def training_dataloader(self, run_no):\n",
    "        return self.trainloader\n",
    "    \n",
    "    def testing_dataloader(self, run_no):\n",
    "        return self.testloader\n",
    "    \n",
    "    def training_step(self, run_no, model):\n",
    "        optim = self.get_optim(run_no)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        def step(tup):\n",
    "            imgs, labels = tup\n",
    "            preds = model(imgs.to(device))\n",
    "            loss = loss_fn(preds, labels.to(device)) ** self.exp\n",
    "            #losses.append(loss.item())\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        return step\n",
    "        \n",
    "    def new_model(self):\n",
    "        return EmbedMNIST(\n",
    "            logic=self.flogic, \n",
    "            lin_channels=self.lin_channels, \n",
    "            logic_layers=self.logic_layers, \n",
    "            logic_width=self.logic_width,\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e08fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTEmbedRegime(TrainingRegime):\n",
    "    def __init__(self, lin_width=16, logic_width=64, logic_layers=1, embed_dims=3, reg_weight=0.01, lr=1e-2, exp=1, no_runs=1):\n",
    "        super().__init__(\"./mnist/\", no_runs)\n",
    "        \n",
    "        self.flogic = logic\n",
    "        self.lin_width = lin_width\n",
    "        self.logic_width = logic_width\n",
    "        self.logic_layers = logic_layers\n",
    "        self.embed_dims = embed_dims\n",
    "        self.reg_weight = reg_weight\n",
    "        self.lr = lr\n",
    "        self.exp = exp\n",
    "        \n",
    "        self.tests = [\n",
    "            CorrectOutputMetric(),\n",
    "            LossMetric(),\n",
    "            EmbedRegMetric(),\n",
    "        ]\n",
    "        \n",
    "        self.trainloader = DataLoader(trainset, batch_size=128)\n",
    "        self.testloader = DataLoader(testset, batch_size=128)\n",
    "        \n",
    "        self.optims = [None] * no_runs\n",
    "        \n",
    "    def get_optim(self, run_no):\n",
    "        optim = self.optims[run_no - 1]\n",
    "        if optim is None:\n",
    "            model = self.get_loaded_model(run_no)\n",
    "            optim = self.optims[run_no - 1] = (\n",
    "                torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "            )\n",
    "        return optim\n",
    "        \n",
    "    def regime_str(self):\n",
    "        return (\n",
    "            \"MNIST | ADAM, LR = %s | EMBEDDED LOGIC, %s DIMS | %s LOGIC REG WEIGHT | WIDTH %s LINEAR LAYER | WIDTH %s LOGIC LAYER | %s LOGIC LAYERS | EXP %s\"\n",
    "            % ( \n",
    "                self.lr,\n",
    "                self.embed_dims,\n",
    "                self.reg_weight,\n",
    "                self.lin_width, \n",
    "                self.logic_width, \n",
    "                self.logic_layers, \n",
    "                self.exp,\n",
    "                \n",
    "            )\n",
    "        ) \n",
    "        \n",
    "    def regime_filename_elems(self):\n",
    "        elems = [\n",
    "            \"mnist\",\n",
    "            str(self.lr),\n",
    "            \"embed\",\n",
    "            \"%slw\" % self.lin_width,\n",
    "            \"%sw\" % self.logic_width,\n",
    "            \"%slyrs\" % self.logic_layers,\n",
    "            \"%sdims\" % self.embed_dims,\n",
    "            \"%sreg\" % self.reg_weight,\n",
    "        ]\n",
    "        if self.exp != 1:\n",
    "            elems.append(\"exp%s\" % self.exp)\n",
    "        return elems\n",
    "        \n",
    "    def training_dataloader(self, run_no):\n",
    "        return self.trainloader\n",
    "    \n",
    "    def testing_dataloader(self, run_no):\n",
    "        return self.testloader\n",
    "    \n",
    "    def training_step(self, run_no, model):\n",
    "        optim = self.get_optim(run_no)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        def step(tup):\n",
    "            imgs, labels = tup\n",
    "            preds = model(imgs.to(device))\n",
    "            loss = (loss_fn(preds, labels.to(device)) ** self.exp) #+ self.reg_weight * model.logic_reg()\n",
    "            if self.reg_weight != 0:\n",
    "                loss += self.reg_weight * model.logic_reg()\n",
    "            \n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            #grads = torch.tensor(0.0)\n",
    "            #for param in model.parameters():\n",
    "            #    with torch.no_grad():\n",
    "            #        print(param.grad.abs().sum().cpu(), param.grad.shape)\n",
    "            #raise Error\n",
    "            #print(grads)\n",
    "            optim.step()\n",
    "            model.zero_reg()\n",
    "            \n",
    "        return step\n",
    "        \n",
    "    def new_model(self):\n",
    "        return EmbedMNIST(\n",
    "            lin_width=self.lin_width, \n",
    "            logic_layers=self.logic_layers, \n",
    "            logic_width=self.logic_width,\n",
    "            embed_dims=self.embed_dims,\n",
    "            reg=self.reg_weight != 0\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3a4ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[MNIST | ADAM, LR = 0.01 | EMBEDDED LOGIC, 5 DIMS | 0.0 LOGIC REG WEIGHT | WIDTH 32 LINEAR LAYER | WIDTH 32 LO…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[MNIST | ADAM, LR = 0.01 | EMBEDDED LOGIC, 5 DIMS | 0.0 LOGIC REG WEIGHT | WIDTH 32 LINEAR LAYER | WIDTH 32 LO…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[MNIST | ADAM, LR = 0.01 | EMBEDDED LOGIC, 5 DIMS | 0.0 LOGIC REG WEIGHT | WIDTH 32 LINEAR LAYER | WIDTH 32 LO…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[MNIST | ADAM, LR = 0.01 | EMBEDDED LOGIC, 5 DIMS | 0.0 LOGIC REG WEIGHT | WIDTH 32 LINEAR LAYER | WIDTH 32 LO…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9734783d7548fc88001bccc1079ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[MNIST | ADAM, LR = 0.01 | EMBEDDED LOGIC, 5 DIMS | 0.0 LOGIC REG WEIGHT | WIDTH 32 LINEAR LAYER | WIDTH 32 LO…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m reg \u001b[38;5;129;01min\u001b[39;00m regs:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m             \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_until\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/chess-ai-backend/src/cache.py:35\u001b[0m, in \u001b[0;36mTrainingRegime.loop_until\u001b[0;34m(self, run_no, until_epoch)\u001b[0m\n\u001b[1;32m     33\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loaded_epoch_count(run_no) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, until_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_until\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_no\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_no\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_and_append_results(run_no)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(run_no)\n",
      "File \u001b[0;32m~/git/chess-ai-backend/src/cache.py:68\u001b[0m, in \u001b[0;36mTrainingRegime.train_until\u001b[0;34m(self, run_no, until_epoch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     step(\u001b[38;5;241m*\u001b[39mloader_args)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_loaded_epoch_count(run_no, epoch_no)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mMNISTEmbedRegime.training_step.<locals>.step\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m     79\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_weight \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mlogic_reg()\n\u001b[1;32m     81\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 82\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#grads = torch.tensor(0.0)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#for param in model.parameters():\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#    with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#        print(param.grad.abs().sum().cpu(), param.grad.shape)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#raise Error\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#print(grads)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def defer_real_regime(*args, **kargs):\n",
    "    def return_regime():\n",
    "        return MNISTRealRegime(*args, **kargs)\n",
    "    return return_regime\n",
    "\n",
    "def defer_embed_regime(*args, **kargs):\n",
    "    def return_regime():\n",
    "        return MNISTEmbedRegime(*args, **kargs)\n",
    "    return return_regime\n",
    "\n",
    "def defer_standard_regime(*args, **kargs):\n",
    "    def return_regime():\n",
    "        return MNISTRegime(*args, **kargs)\n",
    "    return return_regime\n",
    "\n",
    "real_regimes = [\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=32, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=32, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=64, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=64, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=32, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=32, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=64, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=64, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=32, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=32, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=64, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=64, logic_width=64, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=32, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=32, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=64, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=ProductLogic(), lin_channels=256, logic_width=64, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=32, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=32, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=64, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=64, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=32, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=32, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=64, logic_layers=1, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=64, logic_layers=2, keepn=None, lr=1e-2, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=32, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=32, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=64, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=64, logic_width=64, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=32, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=32, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=64, logic_layers=1, keepn=None, lr=1e-3, exp=8),\n",
    "    defer_real_regime(logic=SchweizerSklarLogic(torch.tensor(-2.0).to(device)), lin_channels=256, logic_width=64, logic_layers=2, keepn=None, lr=1e-3, exp=8),\n",
    "]\n",
    "\n",
    "embed_regimes = [\n",
    "    defer_embed_regime(lin_width=32, logic_width=32, logic_layers=2, embed_dims=5, reg_weight=0.000, lr=1e-2, exp=1, no_runs=1)\n",
    "]\n",
    "\n",
    "regimes = [\n",
    "    #defer_standard_regime(lr=1e-3, no_runs=1),\n",
    "    *embed_regimes\n",
    "]\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "if True:\n",
    "    regs = [regime_f() for regime_f in regimes]\n",
    "    for reg in regs:\n",
    "        reg.load_latest_models()\n",
    "        reg.load_all_results()\n",
    "    \n",
    "    for i in range(0, 100):\n",
    "        for reg in regs:\n",
    "            #try:\n",
    "                reg.loop_until(1, 1*(i+1))\n",
    "            #except:\n",
    "            #    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61594c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FuzzyMNIST(logic=ProductLogic(), conv_channels=16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optim, loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        preds = model(imgs.cuda())\n",
    "        loss = loss_fn(preds, labels.cuda())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    return torch.Tensor(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, loader):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        preds = model(imgs.cuda())\n",
    "        pred_labels = preds.argmax(dim=-1)\n",
    "        correct_count += (pred_labels == labels.cuda()).sum()\n",
    "        total_count += labels.numel()\n",
    "        \n",
    "    return correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(model, optim, trainloader, testloader, epochs=1):\n",
    "    losses = torch.zeros(0)\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch_no in range(1, epochs+1):\n",
    "        train_l = tqdm(trainloader, desc=\"Epoch #%s, Training\" % epoch_no, leave=False)\n",
    "        new_losses = train_loop(model, optim, train_l)\n",
    "        losses = torch.cat((losses, new_losses), dim=0)\n",
    "        \n",
    "        test_l = tqdm(testloader, desc=\"Epoch #%s, Testing\" % epoch_no, leave=False)\n",
    "        test_accuracies.append(test_loop(model, test_l))\n",
    "        \n",
    "    return losses, torch.Tensor(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90021e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada92618",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = loop(model, optim, trainloader, testloader, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3,4).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"-\".join([\"0\",\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1] == [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c763c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten([(0,1),(2,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs[0].results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90113711",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs[0].models[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(27).reshape(3,3,3)[(*((slice(None,None,None),)*0),torch.randperm(3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6b7b6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SchweizerSklarLogic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSchweizerSklarLogic\u001b[49m(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10.0\u001b[39m))\u001b[38;5;241m.\u001b[39mbin_conjoin(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SchweizerSklarLogic' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85aa0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
